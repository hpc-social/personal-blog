---
author: Ramblings of a supercomputing enthusiast.
author_tag: gaborsamu
blog_subtitle: Recent content in Blogs on Technical Computing Goulash
blog_title: Blogs on Technical Computing Goulash
blog_url: https://www.gaborsamu.com/blog/
category: gaborsamu
date: '2013-12-20 18:35:22'
layout: post
original_url: https://www.gaborsamu.com/blog/hpc411_nvidia/
slug: ibm-platform-hpc-v4-1-1-1-best-practices-for-managing-nvidia-gpu-devices
title: IBM Platform HPC V4.1.1.1- Best Practices for Managing NVIDIA GPU devices
---

<p><strong>Summary</strong></p>

<p>IBM Platform HPC V4.1.1.1 is easy-to-use, yet comprehensive technical
computing infrastructure management software. It includes as standard GPU
management capabilities including monitoring and workload scheduling for
systems equipped with NVIDIA GPU devices.</p>

<p>At the time of writing, the latest available NVIDIA CUDA version is 5.5. Here
we provide a series of best practices to deploy and manage a IBM Platform HPC
cluster with servers equipped with NVIDIA GPU devices.</p>

<p><strong>Introduction</strong></p>

<p>The document serves as a guide to enabling IBM Platform HPC V4.1.1.1 GPU
management capabilities in a cluster equipped with NVIDIA Tesla Kepler GPUs
and with NVIDIA CUDA 5.5. The steps below assume familiarity with IBM Platform
HPC V4.1.1.1 commands and concepts. As part of the procedure, a NVIDIA CUDA
5.5 Kit is prepared using an example template.</p>

<p>The example cluster defined in the Best Practices below is equipped as follows:</p>

<ul>
<li>Red Hat Enterprise Linux 6.4 (x86-64)</li>
<li>IBM Platform HPC management node (<em>hpc4111tete</em>)</li>
<li>Compute nodes:</li>
<li>compute000 (NVIDIA Tesla K20c)</li>
<li>compute001 (NVIDIA Tesla K40c)</li>
<li>compute002 (NVIDIA Tesla K40c)</li>
</ul>
<p><strong>A. Create a NVIDIA CUDA Kit</strong></p>

<p>It is assumed that the IBM Platform HPC V4.1.1.1 management node (hpctete4111) has been installed and that there are 3 compute nodes equipped with NVIDIA Tesla GPUs that will be provisioned as part of the procedure.</p>

<p>Here we provide the procedure to download the NVIDIA CUDA RPMs from NVIDIA.
This is achieved by installing the NVIDIA CUDA RPM to configure the CUDA
repository from which the NVIDIA CUDA RPMs will be downloaded for packaging as
a Kit.</p>

<p>Additional details regarding IBM Platform HPC Kits can be found <a href="http://sourceforge.net/apps/mediawiki/xcat/index.php?title=Using_Software_Kits_in_OS_Images">here</a>.</p>

<p>The procedure assumes the following:</p>

<ul>
<li>The IBM Platform HPC management node has access to the Internet.</li>
<li>The procedure has been validated with NVIDIA CUDA 5.5</li>
<li>All commands are run as user root on the IBM Platform HPC management node,
unless otherwise indicated.</li>
</ul>
<ol>
<li>Install the yum <em>downloadonly</em> plugin on the IBM Platform HPC management
node.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># yum install yum-plugin-downloadonly</span>
Loaded plugins: product-id, refresh-packagekit, security, subscription-manager
This system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.
Setting up Install Process
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package yum-plugin-downloadonly.noarch 0:1.1.30-14.el6 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

<span style="color: #f92672;">================================================================================</span>
 Package                   Arch     Version         Repository             Size
<span style="color: #f92672;">================================================================================</span>
Installing:
 yum-plugin-downloadonly   noarch   1.1.30-14.el6   xCAT-rhels6.4-path0    <span style="color: #ae81ff;">20</span> k

Transaction Summary
<span style="color: #f92672;">================================================================================</span>
Install       <span style="color: #ae81ff;">1</span> Package<span style="color: #f92672;">(</span>s<span style="color: #f92672;">)</span>

Total download size: <span style="color: #ae81ff;">20</span> k
Installed size: <span style="color: #ae81ff;">21</span> k
Is this ok <span style="color: #f92672;">[</span>y/N<span style="color: #f92672;">]</span>: y
Downloading Packages:
yum-plugin-downloadonly-1.1.30-14.el6.noarch.rpm         |  <span style="color: #ae81ff;">20</span> kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : yum-plugin-downloadonly-1.1.30-14.el6.noarch                 1/1
  Verifying  : yum-plugin-downloadonly-1.1.30-14.el6.noarch                 1/1

Installed:
  yum-plugin-downloadonly.noarch 0:1.1.30-14.el6                                

Complete!</code></pre></div>

<ol start="2">
<li>On the IBM Platform HPC management node, install the NVIDIA CUDA RPM. This
will configure the CUDA repository.</li>
</ol>
<p>Note:  The CUDA RPM can be downloaded <a href="https://developer.nvidia.com/cuda-downloads">here</a>.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># rpm -ivh ./cuda-repo-rhel6-5.5-0.x86_64.rpm</span>
Preparing...                <span style="color: #75715e;">########################################### [100%]</span>
   1:cuda-repo-rhel6        <span style="color: #75715e;">########################################### [100%]</span></code></pre></div>

<ol start="3">
<li>NVIDIA CUDA requires packages which are part of Extra Packages for
Enterprise Linux (EPEL), including <em>dkms</em>. On the IBM Platform HPC management
node, we now install the EPEL repository RPM.</li>
</ol>
<p>Note:  The EPEL RPM for RHEL 6 family can be downloaded <a href="http://download.fedoraproject.org/pub/epel/6/i386/repoview/epel-release.html">here</a>.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># rpm -ivh ./epel-release-6-8.noarch.rpm</span>
warning: ./epel-release-6-8.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID 0608b895: NOKEY
Preparing...                <span style="color: #75715e;">########################################### [100%]</span>
   1:epel-release           <span style="color: #75715e;">########################################### [100%]</span></code></pre></div>

<ol start="4">
<li>Now, the NVIDIA CUDA Toolkit RPMs are downloaded via the OS <em>yum</em> command
to the directory <em>/root/CUDA5.5</em>. The RPMs will be part of the NVIDIA CUDA Kit
which is built in the subsequent steps.</li>
</ol>
<p>Note:  Details on using the <em>yum &ndash;downloadonly</em> option can be found <a href="https://access.redhat.com/site/solutions/10154">here</a>.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># yum install --downloadonly --downloaddir=/root/CUDA5.5/ cuda-5-5.x86_64</span>
Loaded plugins: downloadonly, product-id, refresh-packagekit, security,
              : subscription-manager
This system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.
epel/metalink                                            |  <span style="color: #ae81ff;">15</span> kB     00:00     
epel                                                     | 4.2 kB     00:00     
epel/primary_db                                          | 5.7 MB     00:05     
Setting up Install Process
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package cuda-5-5.x86_64 0:5.5-22 will be installed
--&gt; Processing Dependency: cuda-command-line-tools-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-headers-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-documentation-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-samples-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-visual-tools-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-core-libs-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-license-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-core-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-misc-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-extra-libs-5-5 <span style="color: #f92672;">=</span> 5.5-22 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: xorg-x11-drv-nvidia-devel<span style="color: #f92672;">(</span>x86-32<span style="color: #f92672;">)</span> &gt;<span style="color: #f92672;">=</span> 319.00 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: xorg-x11-drv-nvidia-libs<span style="color: #f92672;">(</span>x86-32<span style="color: #f92672;">)</span> &gt;<span style="color: #f92672;">=</span> 319.00 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: xorg-x11-drv-nvidia-devel<span style="color: #f92672;">(</span>x86-64<span style="color: #f92672;">)</span> &gt;<span style="color: #f92672;">=</span> 319.00 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: nvidia-xconfig &gt;<span style="color: #f92672;">=</span> 319.00 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: cuda-driver &gt;<span style="color: #f92672;">=</span> 319.00 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: nvidia-settings &gt;<span style="color: #f92672;">=</span> 319.00 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Processing Dependency: nvidia-modprobe &gt;<span style="color: #f92672;">=</span> 319.00 <span style="color: #66d9ef;">for</span> package: cuda-5-5-5.5-22.x86_64
--&gt; Running transaction check
---&gt; Package cuda-command-line-tools-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-core-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-core-libs-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-documentation-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-extra-libs-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-headers-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-license-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-misc-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package cuda-samples-5-5.x86_64 0:5.5-22 will be installed
--&gt; Processing Dependency: mesa-libGLU-devel <span style="color: #66d9ef;">for</span> package: cuda-samples-5-5-5.5-22.x86_64
--&gt; Processing Dependency: freeglut-devel <span style="color: #66d9ef;">for</span> package: cuda-samples-5-5-5.5-22.x86_64
--&gt; Processing Dependency: libXi-devel <span style="color: #66d9ef;">for</span> package: cuda-samples-5-5-5.5-22.x86_64
---&gt; Package cuda-visual-tools-5-5.x86_64 0:5.5-22 will be installed
---&gt; Package nvidia-modprobe.x86_64 0:319.37-1.el6 will be installed
---&gt; Package nvidia-settings.x86_64 0:319.37-30.el6 will be installed
---&gt; Package nvidia-xconfig.x86_64 0:319.37-27.el6 will be installed
---&gt; Package xorg-x11-drv-nvidia.x86_64 1:319.37-2.el6 will be installed
--&gt; Processing Dependency: xorg-x11-drv-nvidia-libs<span style="color: #f92672;">(</span>x86-64<span style="color: #f92672;">)</span> <span style="color: #f92672;">=</span> 1:319.37-2.el6 <span style="color: #66d9ef;">for</span> package: 1:xorg-x11-drv-nvidia-319.37-2.el6.x86_64
--&gt; Processing Dependency: nvidia-kmod &gt;<span style="color: #f92672;">=</span> 319.37 <span style="color: #66d9ef;">for</span> package: 1:xorg-x11-drv-nvidia-319.37-2.el6.x86_64
---&gt; Package xorg-x11-drv-nvidia-devel.i686 1:319.37-2.el6 will be installed
---&gt; Package xorg-x11-drv-nvidia-devel.x86_64 1:319.37-2.el6 will be installed
---&gt; Package xorg-x11-drv-nvidia-libs.i686 1:319.37-2.el6 will be installed
--&gt; Processing Dependency: libX11.so.6 <span style="color: #66d9ef;">for</span> package: 1:xorg-x11-drv-nvidia-libs-319.37-2.el6.i686
--&gt; Processing Dependency: libz.so.1 <span style="color: #66d9ef;">for</span> package: 1:xorg-x11-drv-nvidia-libs-319.37-2.el6.i686
--&gt; Processing Dependency: libXext.so.6 <span style="color: #66d9ef;">for</span> package: 1:xorg-x11-drv-nvidia-libs-319.37-2.el6.i686
--&gt; Running transaction check
---&gt; Package freeglut-devel.x86_64 0:2.6.0-1.el6 will be installed
--&gt; Processing Dependency: freeglut <span style="color: #f92672;">=</span> 2.6.0-1.el6 <span style="color: #66d9ef;">for</span> package: freeglut-devel-2.6.0-1.el6.x86_64
--&gt; Processing Dependency: libGL-devel <span style="color: #66d9ef;">for</span> package: freeglut-devel-2.6.0-1.el6.x86_64
--&gt; Processing Dependency: libglut.so.3<span style="color: #f92672;">()(</span>64bit<span style="color: #f92672;">)</span> <span style="color: #66d9ef;">for</span> package: freeglut-devel-2.6.0-1.el6.x86_64
---&gt; Package libX11.i686 0:1.5.0-4.el6 will be installed
--&gt; Processing Dependency: libxcb.so.1 <span style="color: #66d9ef;">for</span> package: libX11-1.5.0-4.el6.i686
---&gt; Package libXext.i686 0:1.3.1-2.el6 will be installed
---&gt; Package libXi-devel.x86_64 0:1.6.1-3.el6 will be installed
---&gt; Package mesa-libGLU-devel.x86_64 0:9.0-0.7.el6 will be installed
---&gt; Package nvidia-kmod.x86_64 1:319.37-1.el6 will be installed
--&gt; Processing Dependency: kernel-devel <span style="color: #66d9ef;">for</span> package: 1:nvidia-kmod-319.37-1.el6.x86_64
--&gt; Processing Dependency: dkms <span style="color: #66d9ef;">for</span> package: 1:nvidia-kmod-319.37-1.el6.x86_64
---&gt; Package xorg-x11-drv-nvidia-libs.x86_64 1:319.37-2.el6 will be installed
---&gt; Package zlib.i686 0:1.2.3-29.el6 will be installed
--&gt; Running transaction check
---&gt; Package dkms.noarch 0:2.2.0.3-17.el6 will be installed
---&gt; Package freeglut.x86_64 0:2.6.0-1.el6 will be installed
---&gt; Package kernel-devel.x86_64 0:2.6.32-358.el6 will be installed
---&gt; Package libxcb.i686 0:1.8.1-1.el6 will be installed
--&gt; Processing Dependency: libXau.so.6 <span style="color: #66d9ef;">for</span> package: libxcb-1.8.1-1.el6.i686
---&gt; Package mesa-libGL-devel.x86_64 0:9.0-0.7.el6 will be installed
--&gt; Processing Dependency: pkgconfig<span style="color: #f92672;">(</span>libdrm<span style="color: #f92672;">)</span> &gt;<span style="color: #f92672;">=</span> 2.4.24 <span style="color: #66d9ef;">for</span> package: mesa-libGL-devel-9.0-0.7.el6.x86_64
--&gt; Processing Dependency: pkgconfig<span style="color: #f92672;">(</span>xxf86vm<span style="color: #f92672;">)</span> <span style="color: #66d9ef;">for</span> package: mesa-libGL-devel-9.0-0.7.el6.x86_64
--&gt; Processing Dependency: pkgconfig<span style="color: #f92672;">(</span>xfixes<span style="color: #f92672;">)</span> <span style="color: #66d9ef;">for</span> package: mesa-libGL-devel-9.0-0.7.el6.x86_64
--&gt; Processing Dependency: pkgconfig<span style="color: #f92672;">(</span>xdamage<span style="color: #f92672;">)</span> <span style="color: #66d9ef;">for</span> package: mesa-libGL-devel-9.0-0.7.el6.x86_64
--&gt; Running transaction check
---&gt; Package libXau.i686 0:1.0.6-4.el6 will be installed
---&gt; Package libXdamage-devel.x86_64 0:1.1.3-4.el6 will be installed
---&gt; Package libXfixes-devel.x86_64 0:5.0-3.el6 will be installed
---&gt; Package libXxf86vm-devel.x86_64 0:1.1.2-2.el6 will be installed
---&gt; Package libdrm-devel.x86_64 0:2.4.39-1.el6 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

<span style="color: #f92672;">================================================================================</span>
 Package                     Arch   Version           Repository           Size
<span style="color: #f92672;">================================================================================</span>
Installing:
 cuda-5-5                    x86_64 5.5-22            cuda                3.3 k
Installing <span style="color: #66d9ef;">for</span> dependencies:
 cuda-command-line-tools-5-5 x86_64 5.5-22            cuda                6.4 M
 cuda-core-5-5               x86_64 5.5-22            cuda                 <span style="color: #ae81ff;">29</span> M
 cuda-core-libs-5-5          x86_64 5.5-22            cuda                <span style="color: #ae81ff;">230</span> k
 cuda-documentation-5-5      x86_64 5.5-22            cuda                 <span style="color: #ae81ff;">79</span> M
 cuda-extra-libs-5-5         x86_64 5.5-22            cuda                <span style="color: #ae81ff;">120</span> M
 cuda-headers-5-5            x86_64 5.5-22            cuda                1.1 M
 cuda-license-5-5            x86_64 5.5-22            cuda                 <span style="color: #ae81ff;">25</span> k
 cuda-misc-5-5               x86_64 5.5-22            cuda                1.7 M
 cuda-samples-5-5            x86_64 5.5-22            cuda                <span style="color: #ae81ff;">150</span> M
 cuda-visual-tools-5-5       x86_64 5.5-22            cuda                <span style="color: #ae81ff;">268</span> M
 dkms                        noarch 2.2.0.3-17.el6    epel                 <span style="color: #ae81ff;">74</span> k
 freeglut                    x86_64 2.6.0-1.el6       xCAT-rhels6.4-path0 <span style="color: #ae81ff;">172</span> k
 freeglut-devel              x86_64 2.6.0-1.el6       xCAT-rhels6.4-path0 <span style="color: #ae81ff;">112</span> k
 kernel-devel                x86_64 2.6.32-358.el6    xCAT-rhels6.4-path0 8.1 M
 libX11                      i686   1.5.0-4.el6       xCAT-rhels6.4-path0 <span style="color: #ae81ff;">590</span> k
 libXau                      i686   1.0.6-4.el6       xCAT-rhels6.4-path0  <span style="color: #ae81ff;">24</span> k
 libXdamage-devel            x86_64 1.1.3-4.el6       xCAT-rhels6.4-path0 9.3 k
 libXext                     i686   1.3.1-2.el6       xCAT-rhels6.4-path0  <span style="color: #ae81ff;">34</span> k
 libXfixes-devel             x86_64 5.0-3.el6         xCAT-rhels6.4-path0  <span style="color: #ae81ff;">12</span> k
 libXi-devel                 x86_64 1.6.1-3.el6       xCAT-rhels6.4-path0 <span style="color: #ae81ff;">102</span> k
 libXxf86vm-devel            x86_64 1.1.2-2.el6       xCAT-rhels6.4-path0  <span style="color: #ae81ff;">17</span> k
 libdrm-devel                x86_64 2.4.39-1.el6      xCAT-rhels6.4-path0  <span style="color: #ae81ff;">77</span> k
 libxcb                      i686   1.8.1-1.el6       xCAT-rhels6.4-path0 <span style="color: #ae81ff;">114</span> k
 mesa-libGL-devel            x86_64 9.0-0.7.el6       xCAT-rhels6.4-path0 <span style="color: #ae81ff;">507</span> k
 mesa-libGLU-devel           x86_64 9.0-0.7.el6       xCAT-rhels6.4-path0 <span style="color: #ae81ff;">111</span> k
 nvidia-kmod                 x86_64 1:319.37-1.el6    cuda                4.0 M
 nvidia-modprobe             x86_64 319.37-1.el6      cuda                 <span style="color: #ae81ff;">14</span> k
 nvidia-settings             x86_64 319.37-30.el6     cuda                <span style="color: #ae81ff;">847</span> k
 nvidia-xconfig              x86_64 319.37-27.el6     cuda                 <span style="color: #ae81ff;">89</span> k
 xorg-x11-drv-nvidia         x86_64 1:319.37-2.el6    cuda                5.1 M
 xorg-x11-drv-nvidia-devel   i686   1:319.37-2.el6    cuda                <span style="color: #ae81ff;">116</span> k
 xorg-x11-drv-nvidia-devel   x86_64 1:319.37-2.el6    cuda                <span style="color: #ae81ff;">116</span> k
 xorg-x11-drv-nvidia-libs    i686   1:319.37-2.el6    cuda                 <span style="color: #ae81ff;">28</span> M
 xorg-x11-drv-nvidia-libs    x86_64 1:319.37-2.el6    cuda                 <span style="color: #ae81ff;">28</span> M
 zlib                        i686   1.2.3-29.el6      xCAT-rhels6.4-path0  <span style="color: #ae81ff;">73</span> k

Transaction Summary
<span style="color: #f92672;">================================================================================</span>
Install      <span style="color: #ae81ff;">36</span> Package<span style="color: #f92672;">(</span>s<span style="color: #f92672;">)</span>

Total download size: <span style="color: #ae81ff;">731</span> M
Installed size: 1.6 G
Is this ok <span style="color: #f92672;">[</span>y/N<span style="color: #f92672;">]</span>: y
Downloading Packages:
<span style="color: #f92672;">(</span>1/36<span style="color: #f92672;">)</span>: cuda-5-5-5.5-22.x86_64.rpm                       | 3.3 kB     00:00     
<span style="color: #f92672;">(</span>2/36<span style="color: #f92672;">)</span>: cuda-command-line-tools-5-5-5.5-22.x86_64.rpm    | 6.4 MB     00:06     
<span style="color: #f92672;">(</span>3/36<span style="color: #f92672;">)</span>: cuda-core-5-5-5.5-22.x86_64.rpm                  |  <span style="color: #ae81ff;">29</span> MB     00:31     
<span style="color: #f92672;">(</span>4/36<span style="color: #f92672;">)</span>: cuda-core-libs-5-5-5.5-22.x86_64.rpm             | <span style="color: #ae81ff;">230</span> kB     00:00     
<span style="color: #f92672;">(</span>5/36<span style="color: #f92672;">)</span>: cuda-documentation-5-5-5.5-22.x86_64.rpm         |  <span style="color: #ae81ff;">79</span> MB     01:28     
<span style="color: #f92672;">(</span>6/36<span style="color: #f92672;">)</span>: cuda-extra-libs-5-5-5.5-22.x86_64.rpm            | <span style="color: #ae81ff;">120</span> MB     02:17     
<span style="color: #f92672;">(</span>7/36<span style="color: #f92672;">)</span>: cuda-headers-5-5-5.5-22.x86_64.rpm               | 1.1 MB     00:01     
<span style="color: #f92672;">(</span>8/36<span style="color: #f92672;">)</span>: cuda-license-5-5-5.5-22.x86_64.rpm               |  <span style="color: #ae81ff;">25</span> kB     00:00     
<span style="color: #f92672;">(</span>9/36<span style="color: #f92672;">)</span>: cuda-misc-5-5-5.5-22.x86_64.rpm                  | 1.7 MB     00:01     
<span style="color: #f92672;">(</span>10/36<span style="color: #f92672;">)</span>: cuda-samples-5-5-5.5-22.x86_64.rpm              | <span style="color: #ae81ff;">150</span> MB     02:51     
<span style="color: #f92672;">(</span>11/36<span style="color: #f92672;">)</span>: cuda-visual-tools-5-5-5.5-22.x86_64.rpm         | <span style="color: #ae81ff;">268</span> MB     05:06     
<span style="color: #f92672;">(</span>12/36<span style="color: #f92672;">)</span>: dkms-2.2.0.3-17.el6.noarch.rpm                  |  <span style="color: #ae81ff;">74</span> kB     00:00     
<span style="color: #f92672;">(</span>13/36<span style="color: #f92672;">)</span>: freeglut-2.6.0-1.el6.x86_64.rpm                 | <span style="color: #ae81ff;">172</span> kB     00:00     
<span style="color: #f92672;">(</span>14/36<span style="color: #f92672;">)</span>: freeglut-devel-2.6.0-1.el6.x86_64.rpm           | <span style="color: #ae81ff;">112</span> kB     00:00     
<span style="color: #f92672;">(</span>15/36<span style="color: #f92672;">)</span>: kernel-devel-2.6.32-358.el6.x86_64.rpm          | 8.1 MB     00:00     
<span style="color: #f92672;">(</span>16/36<span style="color: #f92672;">)</span>: libX11-1.5.0-4.el6.i686.rpm                     | <span style="color: #ae81ff;">590</span> kB     00:00     
<span style="color: #f92672;">(</span>17/36<span style="color: #f92672;">)</span>: libXau-1.0.6-4.el6.i686.rpm                     |  <span style="color: #ae81ff;">24</span> kB     00:00     
<span style="color: #f92672;">(</span>18/36<span style="color: #f92672;">)</span>: libXdamage-devel-1.1.3-4.el6.x86_64.rpm         | 9.3 kB     00:00     
<span style="color: #f92672;">(</span>19/36<span style="color: #f92672;">)</span>: libXext-1.3.1-2.el6.i686.rpm                    |  <span style="color: #ae81ff;">34</span> kB     00:00     
<span style="color: #f92672;">(</span>20/36<span style="color: #f92672;">)</span>: libXfixes-devel-5.0-3.el6.x86_64.rpm            |  <span style="color: #ae81ff;">12</span> kB     00:00     
<span style="color: #f92672;">(</span>21/36<span style="color: #f92672;">)</span>: libXi-devel-1.6.1-3.el6.x86_64.rpm              | <span style="color: #ae81ff;">102</span> kB     00:00     
<span style="color: #f92672;">(</span>22/36<span style="color: #f92672;">)</span>: libXxf86vm-devel-1.1.2-2.el6.x86_64.rpm         |  <span style="color: #ae81ff;">17</span> kB     00:00     
<span style="color: #f92672;">(</span>23/36<span style="color: #f92672;">)</span>: libdrm-devel-2.4.39-1.el6.x86_64.rpm            |  <span style="color: #ae81ff;">77</span> kB     00:00     
<span style="color: #f92672;">(</span>24/36<span style="color: #f92672;">)</span>: libxcb-1.8.1-1.el6.i686.rpm                     | <span style="color: #ae81ff;">114</span> kB     00:00     
<span style="color: #f92672;">(</span>25/36<span style="color: #f92672;">)</span>: mesa-libGL-devel-9.0-0.7.el6.x86_64.rpm         | <span style="color: #ae81ff;">507</span> kB     00:00     
<span style="color: #f92672;">(</span>26/36<span style="color: #f92672;">)</span>: mesa-libGLU-devel-9.0-0.7.el6.x86_64.rpm        | <span style="color: #ae81ff;">111</span> kB     00:00     
<span style="color: #f92672;">(</span>27/36<span style="color: #f92672;">)</span>: nvidia-kmod-319.37-1.el6.x86_64.rpm             | 4.0 MB     00:13     
<span style="color: #f92672;">(</span>28/36<span style="color: #f92672;">)</span>: nvidia-modprobe-319.37-1.el6.x86_64.rpm         |  <span style="color: #ae81ff;">14</span> kB     00:00     
<span style="color: #f92672;">(</span>29/36<span style="color: #f92672;">)</span>: nvidia-settings-319.37-30.el6.x86_64.rpm        | <span style="color: #ae81ff;">847</span> kB     00:01     
<span style="color: #f92672;">(</span>30/36<span style="color: #f92672;">)</span>: nvidia-xconfig-319.37-27.el6.x86_64.rpm         |  <span style="color: #ae81ff;">89</span> kB     00:00     
<span style="color: #f92672;">(</span>31/36<span style="color: #f92672;">)</span>: xorg-x11-drv-nvidia-319.37-2.el6.x86_64.rpm     | 5.1 MB     00:17     
<span style="color: #f92672;">(</span>32/36<span style="color: #f92672;">)</span>: xorg-x11-drv-nvidia-devel-319.37-2.el6.i686.rpm | <span style="color: #ae81ff;">116</span> kB     00:00     
<span style="color: #f92672;">(</span>33/36<span style="color: #f92672;">)</span>: xorg-x11-drv-nvidia-devel-319.37-2.el6.x86_64.r | <span style="color: #ae81ff;">116</span> kB     00:00     
<span style="color: #f92672;">(</span>34/36<span style="color: #f92672;">)</span>: xorg-x11-drv-nvidia-libs-319.37-2.el6.i686.rpm  |  <span style="color: #ae81ff;">28</span> MB     00:31     
<span style="color: #f92672;">(</span>35/36<span style="color: #f92672;">)</span>: xorg-x11-drv-nvidia-libs-319.37-2.el6.x86_64.rp |  <span style="color: #ae81ff;">28</span> MB     02:15     
<span style="color: #f92672;">(</span>36/36<span style="color: #f92672;">)</span>: zlib-1.2.3-29.el6.i686.rpm                      |  <span style="color: #ae81ff;">73</span> kB     00:00     
--------------------------------------------------------------------------------
Total                                           <span style="color: #ae81ff;">786</span> kB/s | <span style="color: #ae81ff;">731</span> MB     15:53     


exiting because --downloadonly specified</code></pre></div>

<ol start="5">
<li>Add <em>dkms</em> as a custom package to the default image profile.  All other
dependencies for NVIDIA CUDA are part of the OS distribution (RHEL 6.4).</li>
</ol>
<p>Note that the <em>plcclient.sh</em> CLI is used here to refresh the imageprofile in
the IBM Platform HPC Web console.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># cp /root/CUDA5.5/dkms-2.2.0.3-20.el6.noarch.rpm /install/contrib/rhels6.4/x86_64/</span>
<span style="color: #75715e;"># plcclient.sh -d "pcmimageprofileloader"</span>
Loaders startup successfully.</code></pre></div>

<ol start="6">
<li>Now we are ready to create the NVIDIA CUDA 5.5 Kit for IBM Platform HPC.<br />
The Kit will contain 3 components:</li>
</ol>
<ul>
<li>NVIDIA Display Driver</li>
<li>NVIDIA CUDA 5.5 Toolkit</li>
<li>NVIDIA CUDA 5.5 Samples and Documentation</li>
</ul>
<p>The <em>buildkit</em> CLI is used to create a new kit template. The template will be
used as the basis for the NVIDIA CUDA 5.5 Kit.  The buildkit CLI is executed
within the directory <em>/root</em>.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># buildkit create kit-CUDA</span>
Kit template <span style="color: #66d9ef;">for</span> kit-CUDA created in /root/kit-CUDA directory</code></pre></div>

<ol start="7">
<li>Copy to <em>/root/kit-CUDA</em> the <em>buildkit.conf</em> file provided in <strong>Appendix A</strong>.<br />
Note that a backup of the original <em>buildkit.conf</em> is performed.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># mv /root/kit-CUDA/buildkit.conf /root/kit-CUDA/buildkit.conf.bak</span>
<span style="color: #75715e;"># cp buildkit.conf /root/kit-CUDA</span></code></pre></div>

<ol start="8">
<li>Copy the NVIDIA RPMs to the kit <em>source_packages</em> directory.  Here we
create subdirectories for the RPMs matching each respective component, then
copy the correct RPMs into place.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># mkdir /root/kit-CUDA/source_packages/cuda-samples</span>
<span style="color: #75715e;"># mkdir /root/kit-CUDA/source_packages/cuda-toolkit</span>
<span style="color: #75715e;"># mkdir /root/kit-CUDA/source_packages/nvidia-driver</span>

<span style="color: #75715e;"># cp /root/CUDA5.5/nvidia-kmod* /root/kit-CUDA/source_packages/nvidia-driver/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/nvidia-modprobe* /root/kit-CUDA/source_packages/nvidia-driver/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/nvidia-settings* /root/kit-CUDA/source_packages/nvidia-driver/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/nvidia-xconfig* /root/kit-CUDA/source_packages/nvidia-driver/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/xorg-x11-drv-nvidia-319.37-2.el6.x86_64* /root/kit-CUDA/source_packages/nvidia-driver/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/xorg-x11-drv-nvidia-devel-319.37-2.el6.x86_64* /root/kit-CUDA/source_packages/nvidia-driver/</span>

<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-command-line-tools* /root/kit-CUDA/source_packages/cuda-toolkit/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-core* /root/kit-CUDA/source_packages/cuda-toolkit/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-extra* /root/kit-CUDA/source_packages/cuda-toolkit/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-headers* /root/kit-CUDA/source_packages/cuda-toolkit/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-license* /root/kit-CUDA/source_packages/cuda-toolkit/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-misc* /root/kit-CUDA/source_packages/cuda-toolkit/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-visual-tools* /root/kit-CUDA/source_packages/cuda-toolkit/</span>

<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-documentation* /root/kit-CUDA/source_packages/cuda-samples/</span>
<span style="color: #75715e;"># cp /root/CUDA5.5/cuda-samples* /root/kit-CUDA/source_packages/cuda-samples/</span></code></pre></div>

<ol start="9">
<li>Copy in place a script required to create a symbolic link <em>/usr/lib64/nvidia/libnvidia-ml.so</em>, required by the LSF <em>elim</em> script for GPUs.  The example
script <em>createsymlink.sh</em> can be found in <strong>Appendix B</strong>.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"> <span style="color: #75715e;"># mkdir /root/kit-CUDA/scripts/nvidia</span>
<span style="color: #75715e;"># cp createsymlink.sh /root/kit-CUDA/scripts/nvidia/</span>
<span style="color: #75715e;"># chmod 755 /root/kit-CUDA/scripts/nvidia/createsymlink.sh</span></code></pre></div>

<ol start="10">
<li>Build the kit repository and the final kit package.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># cd /root/kit-CUDA</span>
<span style="color: #75715e;"># buildkit buildrepo all</span>
Spawning worker <span style="color: #ae81ff;">0</span> with <span style="color: #ae81ff;">22</span> pkgs
Workers Finished
Gathering worker results

Saving Primary metadata
Saving file lists metadata
Saving other metadata
Generating sqlite DBs
Sqlite DBs complete

<span style="color: #75715e;"># buildkit buildtar</span>
Kit tar file /root/kit-CUDA/kit-CUDA-5.5-1.tar.bz2 successfully built.</code></pre></div>

<p><strong>B. Deploying the NVIDIA CUDA 5.5 Kit</strong></p>

<p>In the preceding section, a detailed procedure was provided to download NVIDIA
CUDA 5.5 and to package the NVIDIA CUDA software as a kit for deployment in
cluster managed by IBM Platform HPC.</p>

<p>Here detailed steps are provided to install and deploy the kit. Screenshots
are provided where necessary to illustrate certain operations.</p>

<ol>
<li>In the IBM Platform HPC Web portal, select <em>Resource &gt; Provisioning Templates &gt;Image Profiles</em> and click on Copy to create a copy of the profile <em>rhels6.4-x86_64-stateful-compute</em>. The new image profile will be used for nodes equipped
with NVIDIA GPUs. The new image profile name is <em>rhels6.4-x86_64-stateful-compute_CUDA</em>.</li>
</ol>
<figure><img src="https://www.gaborsamu.com/images/image_profile.png" />
</figure>

<ol start="2">
<li>
<p>Next, we add the CUDA 5.5 Kit to the Kit Library. In the IBM Platform HPC
Web portal, select <em>Resources &gt; Node Provisioning &gt; Kit Library</em> and click Add. Next Browse to the CUDA 5.5 Kit (<em>kit-CUDA-5.5-1.tar.bz2</em>) and add to the Kit
Library.</p>

</li>
<li>
<p>The image profile <em>rhels6.4-x86_64-stateful-compute_CUDA</em> is updated as
follows:</p>

</li>
</ol>
<ul>
<li>select the custom package dkms</li>
<li>select the OS package make</li>
<li>specify Boot Parameters: rdblacklist=nouveau nouveau.modeset=0</li>
<li>enable CUDA Kit components</li>
</ul>
<p>In the IBM Platform HPC Web portal, browse to <em>Resources &gt; Node Provisioning &gt; Provisioning Templates &gt; Image Profiles</em>, select <em>rhels6.4-x86_64-stateful-compute_CUDA</em> and click on Modify.</p>

<p>Under General, specify the Boot Parmaeters <em>rdblacklist=nouveau nouveau.modeset=0</em></p>

<figure><img src="https://www.gaborsamu.com/images/modeset.png" />
</figure>

<p>Under <em>Packages &gt; Custom Packages</em> select <em>dkms.noarch</em>
Under <em>Packages &gt; OS Packages</em> select <em>make</em> (note that the Filter can be used here)</p>

<figure><img src="https://www.gaborsamu.com/images/packages.png" />
</figure>

<p>Under Kit Components select:</p>

<ul>
<li>component-NVIDIA_Driver-5.5-1-rhels-6.4-x86_64</li>
<li>component-CUDA_Samples-5.5-1-rhels-6.4-x86_64</li>
<li>component-CUDA_Toolkit-5.5-1-rhels-6.4-x86_64</li>
</ul>
<p>Note that minimally NVIDIA_Driver and CUDA_Toolkit should be selected.</p>

<figure><img src="https://www.gaborsamu.com/images/components.png" />
</figure>

<ol start="4">
<li>Next, the nodes equipped with NVIDIA GPUs are provisioned. Here Auto
discovery by PXE boot is used to provision the nodes using the newly created
image profile <em>rhels6.4-x86_64-stateful-compute_CUDA</em>.</li>
</ol>
<p>In the IBM Platform HPC Web console select <em>Resources &gt; Devices &gt; Nodes</em> and
click on the Add button.  Here we specify the following provisioning template
properties:</p>

<ul>
<li>Image profile rhels6.4-x86_64-stateful-compute_CUDA</li>
<li>Network profile default_network_profile</li>
<li>Hardware profile IPMI</li>
</ul>
<figure><img src="https://www.gaborsamu.com/images/provision.png" />
</figure>

<p>Then we specify Auto discovery by PXE boot and power on the nodes.  <em>compute000-compute002</em> are provisioned including the NVIDIA CUDA 5.5 Kit components:</p>

<ul>
<li>component-NVIDIA_Driver-5.5-1-rhels-6.4-x86_64</li>
<li>component-CUDA_Samples-5.5-1-rhels-6.4-x86_64</li>
<li>component-CUDA_Toolkit-5.5-1-rhels-6.4-x86_64</li>
</ul>
<figure><img src="https://www.gaborsamu.com/images/provision3.png" />
</figure>

<ol start="5">
<li>After provisioning, check the installation of the NVIDIA Driver and CUDA
stack. <em>xdsh</em> is used here to concurrently execute the NVIDIA CLI <em>nvidia-smi</em>
across the compute nodes.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># xdsh compute00[0-2] nvidia-smi -L</span>
compute000: GPU 0: Tesla K20c <span style="color: #f92672;">(</span>UUID: GPU-46d00ece-a26d-5f8c-c695-23e525a88075<span style="color: #f92672;">)</span>
compute002: GPU 0: Tesla K40c <span style="color: #f92672;">(</span>UUID: GPU-e3ac8955-6a76-12e1-0786-ec336b0b3824<span style="color: #f92672;">)</span>
compute001: GPU 0: Tesla K40c <span style="color: #f92672;">(</span>UUID: GPU-ba2733d8-4473-0a69-af14-e80008568a42<span style="color: #f92672;">)</span></code></pre></div>

<ol start="6">
<li>Next, compile and execute the NVIDIA CUDA <em>deviceQuery</em> sample. Be sure to
check the output for any errors.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"> <span style="color: #75715e;"># xdsh compute00[0-2] -f 1 "cd /usr/local/cuda-5.5/samples/1_Utilities/deviceQuery;make"</span>
compute000: /usr/local/cuda-5.5/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch<span style="color: #f92672;">=</span>compute_10,code<span style="color: #f92672;">=</span>sm_10 -gencode arch<span style="color: #f92672;">=</span>compute_20,code<span style="color: #f92672;">=</span>sm_20 -gencode arch<span style="color: #f92672;">=</span>compute_30,code<span style="color: #f92672;">=</span>sm_30 -gencode arch<span style="color: #f92672;">=</span>compute_35,code<span style="color: #f92672;">=</span><span style="color: #ae81ff;">\"</span>sm_35,compute_35<span style="color: #ae81ff;">\"</span> -o deviceQuery.o -c deviceQuery.cpp
compute000: /usr/local/cuda-5.5/bin/nvcc -ccbin g++   -m64        -o deviceQuery deviceQuery.o
compute000: mkdir -p ../../bin/x86_64/linux/release
compute000: cp deviceQuery ../../bin/x86_64/linux/release
compute001: /usr/local/cuda-5.5/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch<span style="color: #f92672;">=</span>compute_10,code<span style="color: #f92672;">=</span>sm_10 -gencode arch<span style="color: #f92672;">=</span>compute_20,code<span style="color: #f92672;">=</span>sm_20 -gencode arch<span style="color: #f92672;">=</span>compute_30,code<span style="color: #f92672;">=</span>sm_30 -gencode arch<span style="color: #f92672;">=</span>compute_35,code<span style="color: #f92672;">=</span><span style="color: #ae81ff;">\"</span>sm_35,compute_35<span style="color: #ae81ff;">\"</span> -o deviceQuery.o -c deviceQuery.cpp
compute001: /usr/local/cuda-5.5/bin/nvcc -ccbin g++   -m64        -o deviceQuery deviceQuery.o
compute001: mkdir -p ../../bin/x86_64/linux/release
compute001: cp deviceQuery ../../bin/x86_64/linux/release
compute002: /usr/local/cuda-5.5/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch<span style="color: #f92672;">=</span>compute_10,code<span style="color: #f92672;">=</span>sm_10 -gencode arch<span style="color: #f92672;">=</span>compute_20,code<span style="color: #f92672;">=</span>sm_20 -gencode arch<span style="color: #f92672;">=</span>compute_30,code<span style="color: #f92672;">=</span>sm_30 -gencode arch<span style="color: #f92672;">=</span>compute_35,code<span style="color: #f92672;">=</span><span style="color: #ae81ff;">\"</span>sm_35,compute_35<span style="color: #ae81ff;">\"</span> -o deviceQuery.o -c deviceQuery.cpp
compute002: /usr/local/cuda-5.5/bin/nvcc -ccbin g++   -m64        -o deviceQuery deviceQuery.o
compute002: mkdir -p ../../bin/x86_64/linux/release
compute002: cp deviceQuery ../../bin/x86_64/linux/release

<span style="color: #75715e;"># xdsh compute00[0-2] -f 1 /usr/local/cuda-5.5/samples/1_Utilities/deviceQuery/deviceQuery</span>
compute000: /usr/local/cuda-5.5/samples/1_Utilities/deviceQuery/deviceQuery Starting...
compute000:
compute000:  CUDA Device Query <span style="color: #f92672;">(</span>Runtime API<span style="color: #f92672;">)</span> version <span style="color: #f92672;">(</span>CUDART static linking<span style="color: #f92672;">)</span>
compute000:
compute000: Detected <span style="color: #ae81ff;">1</span> CUDA Capable device<span style="color: #f92672;">(</span>s<span style="color: #f92672;">)</span>
compute000:
compute000: Device 0: <span style="color: #e6db74;">"Tesla K20c"</span>
compute000:   CUDA Driver Version / Runtime Version          5.5 / 5.5
compute000:   CUDA Capability Major/Minor version number:    3.5
compute000:   Total amount of global memory:                 <span style="color: #ae81ff;">4800</span> MBytes <span style="color: #f92672;">(</span><span style="color: #ae81ff;">5032706048</span> bytes<span style="color: #f92672;">)</span>
compute000:   <span style="color: #f92672;">(</span>13<span style="color: #f92672;">)</span> Multiprocessors, <span style="color: #f92672;">(</span>192<span style="color: #f92672;">)</span> CUDA Cores/MP:     <span style="color: #ae81ff;">2496</span> CUDA Cores
compute000:   GPU Clock rate:                                <span style="color: #ae81ff;">706</span> MHz <span style="color: #f92672;">(</span>0.71 GHz<span style="color: #f92672;">)</span>
compute000:   Memory Clock rate:                             <span style="color: #ae81ff;">2600</span> Mhz
compute000:   Memory Bus Width:                              320-bit
compute000:   L2 Cache Size:                                 <span style="color: #ae81ff;">1310720</span> bytes
compute000:   Maximum Texture Dimension Size <span style="color: #f92672;">(</span>x,y,z<span style="color: #f92672;">)</span>         1D<span style="color: #f92672;">=(</span>65536<span style="color: #f92672;">)</span>, 2D<span style="color: #f92672;">=(</span>65536, 65536<span style="color: #f92672;">)</span>, 3D<span style="color: #f92672;">=(</span>4096, 4096, 4096<span style="color: #f92672;">)</span>
compute000:   Maximum Layered 1D Texture Size, <span style="color: #f92672;">(</span>num<span style="color: #f92672;">)</span> layers  1D<span style="color: #f92672;">=(</span>16384<span style="color: #f92672;">)</span>, <span style="color: #ae81ff;">2048</span> layers
compute000:   Maximum Layered 2D Texture Size, <span style="color: #f92672;">(</span>num<span style="color: #f92672;">)</span> layers  2D<span style="color: #f92672;">=(</span>16384, 16384<span style="color: #f92672;">)</span>, <span style="color: #ae81ff;">2048</span> layers
compute000:   Total amount of constant memory:               <span style="color: #ae81ff;">65536</span> bytes
compute000:   Total amount of shared memory per block:       <span style="color: #ae81ff;">49152</span> bytes
compute000:   Total number of registers available per block: <span style="color: #ae81ff;">65536</span>
compute000:   Warp size:                                     <span style="color: #ae81ff;">32</span>
compute000:   Maximum number of threads per multiprocessor:  <span style="color: #ae81ff;">2048</span>
compute000:   Maximum number of threads per block:           <span style="color: #ae81ff;">1024</span>
compute000:   Max dimension size of a thread block <span style="color: #f92672;">(</span>x,y,z<span style="color: #f92672;">)</span>: <span style="color: #f92672;">(</span>1024, 1024, 64<span style="color: #f92672;">)</span>
compute000:   Max dimension size of a grid size    <span style="color: #f92672;">(</span>x,y,z<span style="color: #f92672;">)</span>: <span style="color: #f92672;">(</span>2147483647, 65535, 65535<span style="color: #f92672;">)</span>
compute000:   Maximum memory pitch:                          <span style="color: #ae81ff;">2147483647</span> bytes
compute000:   Texture alignment:                             <span style="color: #ae81ff;">512</span> bytes
compute000:   Concurrent copy and kernel execution:          Yes with <span style="color: #ae81ff;">2</span> copy engine<span style="color: #f92672;">(</span>s<span style="color: #f92672;">)</span>
compute000:   Run time limit on kernels:                     No
compute000:   Integrated GPU sharing Host Memory:            No
compute000:   Support host page-locked memory mapping:       Yes
compute000:   Alignment requirement <span style="color: #66d9ef;">for</span> Surfaces:            Yes
compute000:   Device has ECC support:                        Enabled
compute000:   Device supports Unified Addressing <span style="color: #f92672;">(</span>UVA<span style="color: #f92672;">)</span>:      Yes
compute000:   Device PCI Bus ID / PCI location ID:           <span style="color: #ae81ff;">10</span> / <span style="color: #ae81ff;">0</span>
compute000:   Compute Mode:
compute000:      &lt; Default <span style="color: #f92672;">(</span>multiple host threads can use ::cudaSetDevice<span style="color: #f92672;">()</span> with device simultaneously<span style="color: #f92672;">)</span> &gt;
compute000:
compute000: deviceQuery, CUDA Driver <span style="color: #f92672;">=</span> CUDART, CUDA Driver Version <span style="color: #f92672;">=</span> 5.5, CUDA Runtime Version <span style="color: #f92672;">=</span> 5.5, NumDevs <span style="color: #f92672;">=</span> 1, Device0 <span style="color: #f92672;">=</span> Tesla K20c
compute000: Result <span style="color: #f92672;">=</span> PASS
compute001: /usr/local/cuda-5.5/samples/1_Utilities/deviceQuery/deviceQuery Starting...
....

....

compute002: /usr/local/cuda-5.5/samples/1_Utilities/deviceQuery/deviceQuery Starting...
....

....

compute002: deviceQuery, CUDA Driver <span style="color: #f92672;">=</span> CUDART, CUDA Driver Version <span style="color: #f92672;">=</span> 5.5, CUDA Runtime Version <span style="color: #f92672;">=</span> 5.5, NumDevs <span style="color: #f92672;">=</span> 1, Device0 <span style="color: #f92672;">=</span> Tesla K40c
compute002: Result <span style="color: #f92672;">=</span> PASS</code></pre></div>

<p>Up to this point, we have produced a CUDA 5.5 Kit, and deployed nodes including
CUDA 5.5. Furthermore, we have tested the correct installation of the NVIDIA
driver as well as the CUDA Sample <em>deviceQuery</em> example.</p>

<p><strong>C. Enable Management of NVIDIA GPU devices</strong></p>

<p>The IBM Platform HPC Administration Guide contains detailed steps on enabling
NVIDIA GPU monitoring. This can be found in Chapter 10 in the section titled
<em>Enabling the GPU</em>.</p>

<p>Below, as the user root, we make the necessary updates to the IBM Platform
HPC (and workload manager) configuration files to enable LSF GPU support.</p>

<ol>
<li>Define the GPU processing resources reported by ELIM in <em>$LSF_ENVDIR/lsf.shared</em>. You can define a new resources section to include the following resource
definitions:</li>
</ol>
<div class="highlight"><pre><code class="language-bash">Begin Resource
RESOURCENAME TYPE INTERVAL INCREASING CONSUMABLE DESCRIPTION <span style="color: #75715e;"># Keywords</span>
ngpus Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Number of GPUs<span style="color: #f92672;">)</span>
gpushared Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Number of GPUs in Shared Mode<span style="color: #f92672;">)</span>
gpuexcl_thrd Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Number of GPUs in Exclusive thread Mode<span style="color: #f92672;">)</span>
gpuprohibited Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Number of GPUs in Prohibited Mode<span style="color: #f92672;">)</span>
gpuexcl_proc Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Number of GPUs in Exclusive process Mode<span style="color: #f92672;">)</span>
gpumode0 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Mode of 1st GPU<span style="color: #f92672;">)</span>
gputemp0 Numeric <span style="color: #ae81ff;">60</span> Y <span style="color: #f92672;">(</span>Temperature of 1st GPU<span style="color: #f92672;">)</span>
gpuecc0 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>ECC errors on 1st GPU<span style="color: #f92672;">)</span>
gpumode1 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Mode of 2nd GPU<span style="color: #f92672;">)</span>
gputemp1 Numeric <span style="color: #ae81ff;">60</span> Y <span style="color: #f92672;">(</span>Temperature of 2nd GPU<span style="color: #f92672;">)</span>
gpuecc1 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>ECC errors on 2nd GPU<span style="color: #f92672;">)</span>
gpumode2 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Mode of 3rd GPU<span style="color: #f92672;">)</span>
gputemp2 Numeric <span style="color: #ae81ff;">60</span> Y <span style="color: #f92672;">(</span>Temperature of 3rd GPU<span style="color: #f92672;">)</span>
gpuecc2 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>ECC errors on 3rd GPU<span style="color: #f92672;">)</span>
gpumode3 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>Mode of 4th GPU<span style="color: #f92672;">)</span>
gputemp3 Numeric <span style="color: #ae81ff;">60</span> Y <span style="color: #f92672;">(</span>Temperature of 4th GPU<span style="color: #f92672;">)</span>
gpuecc3 Numeric <span style="color: #ae81ff;">60</span> N <span style="color: #f92672;">(</span>ECC errors on 4th GPU<span style="color: #f92672;">)</span>
gpudriver String <span style="color: #ae81ff;">60</span> <span style="color: #f92672;">()</span> <span style="color: #f92672;">(</span>GPU driver version<span style="color: #f92672;">)</span>
gpumodel0 String <span style="color: #ae81ff;">60</span> <span style="color: #f92672;">()</span> <span style="color: #f92672;">(</span>Model name of 1st GPU<span style="color: #f92672;">)</span>
gpumodel1 String <span style="color: #ae81ff;">60</span> <span style="color: #f92672;">()</span> <span style="color: #f92672;">(</span>Model name of 2nd GPU<span style="color: #f92672;">)</span>
gpumodel2 String <span style="color: #ae81ff;">60</span> <span style="color: #f92672;">()</span> <span style="color: #f92672;">(</span>Model name of 3rd GPU<span style="color: #f92672;">)</span>
gpumodel3 String <span style="color: #ae81ff;">60</span> <span style="color: #f92672;">()</span> <span style="color: #f92672;">(</span>Model name of 4th GPU<span style="color: #f92672;">)</span>
End Resource</code></pre></div>

<ol start="2">
<li>Define the following resource map to support GPU processing in <em>$LSF_ENVDIR/lsf.cluster.cluster-name</em>, where <em>cluster-name</em> is the name of your cluster.</li>
</ol>
<div class="highlight"><pre><code class="language-bash">Begin ResourceMap
RESOURCENAME LOCATION
ngpus <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpushared <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpuexcl_thrd <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpuprohibited <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpuexcl_proc <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumode0 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gputemp0 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpuecc0 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumode1 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gputemp1 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpuecc1 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumode2 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gputemp2 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpuecc2 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumode3 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gputemp3 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpuecc3 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumodel0 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumodel1 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumodel2 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpumodel3 <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
gpudriver <span style="color: #f92672;">[</span>default<span style="color: #f92672;">]</span>
End ResourceMap</code></pre></div>

<ol start="3">
<li>To configure the newly defined resources, run the following command on the
IBM Platform HPC management node. Here we must restart the LIMs on all hosts.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># lsadmin reconfig</span>

Checking configuration files ...
No errors found.

Restart only the master candidate hosts? <span style="color: #f92672;">[</span>y/n<span style="color: #f92672;">]</span> n
Do you really want to restart LIMs on all hosts? <span style="color: #f92672;">[</span>y/n<span style="color: #f92672;">]</span> y
Restart LIM on &lt;hpc4111tete&gt; ...... <span style="color: #66d9ef;">done</span>
Restart LIM on &lt;compute000&gt; ...... <span style="color: #66d9ef;">done</span>
Restart LIM on &lt;compute001&gt; ...... <span style="color: #66d9ef;">done</span>
Restart LIM on &lt;compute002&gt; ...... <span style="color: #66d9ef;">done</span></code></pre></div>

<ol start="4">
<li>Define how NVIDA CUDA jobs are submitted to LSF in <em>$LSF_ENVDIR/lsbatch/cluster-name/configdir/lsb.applications</em>, where <em>cluster-name</em> is the name of your
cluster.</li>
</ol>
<div class="highlight"><pre><code class="language-bash">Begin Application
NAME <span style="color: #f92672;">=</span> nvjobsh
DESCRIPTION <span style="color: #f92672;">=</span> NVIDIA Shared GPU Jobs
JOB_STARTER <span style="color: #f92672;">=</span> nvjob <span style="color: #e6db74;">"%USRCMD"</span>
RES_REQ <span style="color: #f92672;">=</span> <span style="color: #66d9ef;">select</span><span style="color: #f92672;">[</span>gpushared&gt;0<span style="color: #f92672;">]</span>
End Application

Begin Application
NAME <span style="color: #f92672;">=</span> nvjobex_t
DESCRIPTION <span style="color: #f92672;">=</span> NVIDIA Exclusive GPU Jobs
JOB_STARTER <span style="color: #f92672;">=</span> nvjob <span style="color: #e6db74;">"%USRCMD"</span>
RES_REQ <span style="color: #f92672;">=</span> rusage<span style="color: #f92672;">[</span>gpuexcl_thrd<span style="color: #f92672;">=</span>1<span style="color: #f92672;">]</span>
End Application

Begin Application
NAME <span style="color: #f92672;">=</span> nvjobex2_t
DESCRIPTION <span style="color: #f92672;">=</span> NVIDIA Exclusive GPU Jobs
JOB_STARTER <span style="color: #f92672;">=</span> nvjob <span style="color: #e6db74;">"%USRCMD"</span>
RES_REQ <span style="color: #f92672;">=</span> rusage<span style="color: #f92672;">[</span>gpuexcl_thrd<span style="color: #f92672;">=</span>2<span style="color: #f92672;">]</span>
End Application

Begin Application
NAME <span style="color: #f92672;">=</span> nvjobex_p
DESCRIPTION <span style="color: #f92672;">=</span> NVIDIA Exclusive-process GPU Jobs
JOB_STARTER <span style="color: #f92672;">=</span> nvjob <span style="color: #e6db74;">"%USRCMD"</span>
RES_REQ <span style="color: #f92672;">=</span> rusage<span style="color: #f92672;">[</span>gpuexcl_proc<span style="color: #f92672;">=</span>1<span style="color: #f92672;">]</span>
End Application

Begin Application
NAME <span style="color: #f92672;">=</span> nvjobex2_p
DESCRIPTION <span style="color: #f92672;">=</span> NVIDIA Exclusive-process GPU Jobs
JOB_STARTER <span style="color: #f92672;">=</span> nvjob <span style="color: #e6db74;">"%USRCMD"</span>
RES_REQ <span style="color: #f92672;">=</span> rusage<span style="color: #f92672;">[</span>gpuexcl_proc<span style="color: #f92672;">=</span>2<span style="color: #f92672;">]</span>
End Application</code></pre></div>

<ol start="5">
<li>To add the GPU-related application pofiles, issue the following command on
the IBM Platform HPC management node.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># badmin reconfig</span>

Checking configuration files ...

No errors found.

Reconfiguration initiated</code></pre></div>

<ol start="6">
<li>To enable monitoring GPU related metrics in the IBM Platform HPC Web portal,
modify the <em>$PMC_TOP/gui/conf/pmc.conf</em> configuration file by setting the
variable <em>ENABLE_GPU_MONITORING</em> equal to <em>Y</em>. To make the change take effect,
it is necessary to restart the IBM Platform HPC Web portal server.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># service pmc stop</span>
<span style="color: #75715e;"># service pmc start</span></code></pre></div>

<ol start="7">
<li>Within the IBM Platform HPC Web portal, the GPU tab is now present for
nodes equipped with NVIDIA GPUs. Browse to <em>Resources &gt; Devices &gt; Nodes</em> and
select the GPU tab.</li>
</ol>
<figure><img src="https://www.gaborsamu.com/images/gputab.png" />
</figure>

<p><strong>D. Job Submission - specifying GPU resources</strong></p>

<p>The IBM Platform HPC Administration Guide contains detailed steps job submission
to GPU resources.  This can be found in Chapter 10 in the section titled
<em>Submitting jobs to a cluster by specifying GPU resources</em>. Please refer to the
IBM Platform HPC Administration Guide for detailed information.</p>

<p>Here we look at a simple example. In Section C. we configured a number of
application profiles specific to GPU jobs.</p>

<ol>
<li>The list of application profiles can be displayed using the <em>bapp</em> CLI:</li>
</ol>
<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># bapp</span>
APP_NAME            NJOBS     PEND      RUN     SUSP
nvjobex2_p              <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>
nvjobex2_t              <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>
nvjobex_p               <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>
nvjobex_t               <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>
nvjobsh                 <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span></code></pre></div>

<p>Furthermore, details regarding a specific profile can be obtained as follows:</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># bapp -l nvjobsh</span>

APPLICATION NAME: nvjobsh
 -- NVIDIA Shared GPU Jobs

STATISTICS:
   NJOBS     PEND      RUN    SSUSP    USUSP      RSV
       <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>        <span style="color: #ae81ff;">0</span>

PARAMETERS:

JOB_STARTER: nvjob <span style="color: #e6db74;">"%USRCMD"</span>
RES_REQ: <span style="color: #66d9ef;">select</span><span style="color: #f92672;">[</span>gpushared&gt;0<span style="color: #f92672;">]</span></code></pre></div>

<ol start="2">
<li>To submit a job for execution on a GPU resource, the folloiwng syntax is
used. Note here that the <em>deviceQuery</em> application (previously compiled) is
submitted for execution). The following command is run as user <em>phpcadmin</em>.<br />
The <em>bsub -a</em> option is used to specify the application profile at the time of
job submission.</li>
</ol>
<div class="highlight"><pre><code class="language-bash"> $ bsub -I -a gpushared /usr/local/cuda-5.5/samples/1_Utilities/deviceQuery/deviceQuery
Job &lt;208&gt; is submitted to default queue &lt;medium_priority&gt;.
<span style="color: #e6db74;">&lt;&lt;Waiting for dispatch ...&gt;&gt;
</span><span style="color: #e6db74;">&lt;&lt;Starting on compute000&gt;&gt;
</span><span style="color: #e6db74;">/usr/local/cuda-5.5/samples/1_Utilities/deviceQuery/deviceQuery Starting...
</span><span style="color: #e6db74;">
</span><span style="color: #e6db74;"> CUDA Device Query (Runtime API) version (CUDART static linking)
</span><span style="color: #e6db74;">
</span><span style="color: #e6db74;">Detected 1 CUDA Capable device(s)
</span><span style="color: #e6db74;">
</span><span style="color: #e6db74;">Device 0: "Tesla K20c"
</span><span style="color: #e6db74;">  CUDA Driver Version / Runtime Version          5.5 / 5.5
</span><span style="color: #e6db74;">  CUDA Capability Major/Minor version number:    3.5
</span><span style="color: #e6db74;">  Total amount of global memory:                 4800 MBytes (5032706048 bytes)
</span><span style="color: #e6db74;">  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores
</span><span style="color: #e6db74;">  GPU Clock rate:                                706 MHz (0.71 GHz)
</span><span style="color: #e6db74;">  Memory Clock rate:                             2600 Mhz
</span><span style="color: #e6db74;">  Memory Bus W</span>idth:                              320-bit
  L2 Cache Size:                                 <span style="color: #ae81ff;">1310720</span> bytes
  Maximum Texture Dimension Size <span style="color: #f92672;">(</span>x,y,z<span style="color: #f92672;">)</span>         1D<span style="color: #f92672;">=(</span>65536<span style="color: #f92672;">)</span>, 2D<span style="color: #f92672;">=(</span>65536, 65536<span style="color: #f92672;">)</span>, 3D<span style="color: #f92672;">=(</span>4096, 4096, 4096<span style="color: #f92672;">)</span>
  Maximum Layered 1D Texture Size, <span style="color: #f92672;">(</span>num<span style="color: #f92672;">)</span> layers  1D<span style="color: #f92672;">=(</span>16384<span style="color: #f92672;">)</span>, <span style="color: #ae81ff;">2048</span> layers
  Maximum Layered 2D Texture Size, <span style="color: #f92672;">(</span>num<span style="color: #f92672;">)</span> layers  2D<span style="color: #f92672;">=(</span>16384, 16384<span style="color: #f92672;">)</span>, <span style="color: #ae81ff;">2048</span> layers
  Total amount of constant memory:               <span style="color: #ae81ff;">65536</span> bytes
  Total amount of shared memory per block:       <span style="color: #ae81ff;">49152</span> bytes
  Total number of registers available per block: <span style="color: #ae81ff;">65536</span>
....

....

  Compute Mode:
     &lt; Default <span style="color: #f92672;">(</span>multiple host threads can use ::cudaSetDevice<span style="color: #f92672;">()</span> with device simultaneously<span style="color: #f92672;">)</span> &gt;

deviceQuery, CUDA Driver <span style="color: #f92672;">=</span> CUDART, CUDA Driver Version <span style="color: #f92672;">=</span> 5.5, CUDA Runtime Version <span style="color: #f92672;">=</span> 5.5, NumDevs <span style="color: #f92672;">=</span> 1, Device0 <span style="color: #f92672;">=</span> Tesla K20c
Result <span style="color: #f92672;">=</span> PASS</code></pre></div>

<p><strong>Appendix A: Example NVIDIA CUDA 5.5 Kit template (buildkit.conf)</strong></p>

<div class="highlight"><pre><code class="language-bash"> <span style="color: #75715e;"># Copyright International Business Machine Corporation, 2012-2013</span>

<span style="color: #75715e;"># This information contains sample application programs in source language, which</span>
<span style="color: #75715e;"># illustrates programming techniques on various operating platforms. You may copy,</span>
<span style="color: #75715e;"># modify, and distribute these sample programs in any form without payment to IBM,</span>
<span style="color: #75715e;"># for the purposes of developing, using, marketing or distributing application</span>
<span style="color: #75715e;"># programs conforming to the application programming interface for the operating</span>
<span style="color: #75715e;"># platform for which the sample programs are written. These examples have not been</span>
<span style="color: #75715e;"># thoroughly tested under all conditions. IBM, therefore, cannot guarantee or</span>
<span style="color: #75715e;"># imply reliability, serviceability, or function of these programs. The sample</span>
<span style="color: #75715e;"># programs are provided "AS IS", without warranty of any kind. IBM shall not be</span>
<span style="color: #75715e;"># liable for any damages arising out of your use of the sample programs.</span>

<span style="color: #75715e;"># Each copy or any portion of these sample programs or any derivative work, must</span>
<span style="color: #75715e;"># include a copyright notice as follows:</span>

<span style="color: #75715e;"># (C) Copyright IBM Corp. 2012-2013.</span>

<span style="color: #75715e;"># Kit Build File</span>
<span style="color: #75715e;">#</span>
<span style="color: #75715e;"># Copyright International Business Machine Corporation, 2012-2013</span>
<span style="color: #75715e;">#</span>
<span style="color: #75715e;"># This example buildkit.conf file may be used to build a NVIDIA CUDA 5.5 Kit</span>
<span style="color: #75715e;"># for Red Hat Enterprise Linux 6.4 (x64).  </span>
<span style="color: #75715e;"># The Kit will be comprised of 3 components allowing you to install:</span>
<span style="color: #75715e;"># 1.  NVIDIA Display Driver</span>
<span style="color: #75715e;"># 2.  NVIDIA CUDA 5.5 Toolkit</span>
<span style="color: #75715e;"># 3.  NVIDIA CUDA 5.5 Samples and Documentation</span>
<span style="color: #75715e;">#</span>
<span style="color: #75715e;"># Refer to the buildkit manpage for further details.</span>
<span style="color: #75715e;">#</span>
kit:
   basename<span style="color: #f92672;">=</span>kit-CUDA
   description<span style="color: #f92672;">=</span>NVIDIA CUDA 5.5 Kit          
   version<span style="color: #f92672;">=</span>5.5
   release<span style="color: #f92672;">=</span><span style="color: #ae81ff;">1</span>
   ostype<span style="color: #f92672;">=</span>Linux
   kitlicense<span style="color: #f92672;">=</span>Proprietary


kitrepo:
   kitrepoid<span style="color: #f92672;">=</span>rhels6.4
   osbasename<span style="color: #f92672;">=</span>rhels
   osmajorversion<span style="color: #f92672;">=</span><span style="color: #ae81ff;">6</span>
   osminorversion<span style="color: #f92672;">=</span><span style="color: #ae81ff;">4</span>
   osarch<span style="color: #f92672;">=</span>x86_64


kitcomponent:
    basename<span style="color: #f92672;">=</span>component-NVIDIA_Driver
    description<span style="color: #f92672;">=</span>NVIDIA Display Driver 319.37
    serverroles<span style="color: #f92672;">=</span>mgmt,compute
    ospkgdeps<span style="color: #f92672;">=</span>make
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    kitpkgdeps<span style="color: #f92672;">=</span>nvidia-kmod,nvidia-modprobe,nvidia-settings,nvidia-xconfig,xorg-x11-drv-nvidia,xorg-x11-drv-nvidia-devel,xorg-x11-drv-nvidia-libs
    postinstall<span style="color: #f92672;">=</span>nvidia/createsymlink.sh

kitcomponent:
    basename<span style="color: #f92672;">=</span>component-CUDA_Toolkit
    description<span style="color: #f92672;">=</span>NVIDIA CUDA 5.5 Toolkit 5.5-22
    serverroles<span style="color: #f92672;">=</span>mgmt,compute
    ospkgdeps<span style="color: #f92672;">=</span>gcc-c++
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    kitpkgdeps<span style="color: #f92672;">=</span>cuda-command-line-tools,cuda-core,cuda-core-libs,cuda-extra-libs,cuda-headers,cuda-license,cuda-misc,cuda-visual-tools

kitcomponent:
    basename<span style="color: #f92672;">=</span>component-CUDA_Samples  
    description<span style="color: #f92672;">=</span>NVIDIA CUDA 5.5 Samples and Documentation 5.5-22
    serverroles<span style="color: #f92672;">=</span>mgmt,compute
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    kitpkgdeps<span style="color: #f92672;">=</span>cuda-documentation,cuda-samples


kitpackage:
    filename<span style="color: #f92672;">=</span>nvidia-kmod-*.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no
    rpm_prebuiltdir<span style="color: #f92672;">=</span>nvidia-driver

kitpackage:
    filename<span style="color: #f92672;">=</span>nvidia-modprobe-*.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no
    rpm_prebuiltdir<span style="color: #f92672;">=</span>nvidia-driver

kitpackage:
    filename<span style="color: #f92672;">=</span>nvidia-settings-*.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>nvidia-driver

kitpackage:
    filename<span style="color: #f92672;">=</span>nvidia-xconfig-*.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>nvidia-driver

kitpackage:
    filename<span style="color: #f92672;">=</span>xorg-x11-drv-nvidia-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>nvidia-driver

kitpackage:
    filename<span style="color: #f92672;">=</span>xorg-x11-drv-nvidia-devel-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>nvidia-driver

kitpackage:
    filename<span style="color: #f92672;">=</span>xorg-x11-drv-nvidia-libs-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>nvidia-driver

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-command-line-tools-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no   
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-core-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-core-libs-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-extra-libs-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-headers-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-license-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-misc-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-visual-tools-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-toolkit

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-documentation-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-samples

kitpackage:
    filename<span style="color: #f92672;">=</span>cuda-samples-*.x86_64.rpm
    kitrepoid<span style="color: #f92672;">=</span>rhels6.4
    <span style="color: #75715e;"># Method 1: Use pre-built RPM package</span>
    isexternalpkg<span style="color: #f92672;">=</span>no  
    rpm_prebuiltdir<span style="color: #f92672;">=</span>cuda-samples</code></pre></div>

<p><strong>Appendix B: createsymlink.sh</strong></p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># Copyright International Business Machine Corporation, 2012-2013</span>

<span style="color: #75715e;"># This information contains sample application programs in source language, which</span>
<span style="color: #75715e;"># illustrates programming techniques on various operating platforms. You may copy,</span>
<span style="color: #75715e;"># modify, and distribute these sample programs in any form without payment to IBM,</span>
<span style="color: #75715e;"># for the purposes of developing, using, marketing or distributing application</span>
<span style="color: #75715e;"># programs conforming to the application programming interface for the operating</span>
<span style="color: #75715e;"># platform for which the sample programs are written. These examples have not been</span>
<span style="color: #75715e;"># thoroughly tested under all conditions. IBM, therefore, cannot guarantee or</span>
<span style="color: #75715e;"># imply reliability, serviceability, or function of these programs. The sample</span>
<span style="color: #75715e;"># programs are provided "AS IS", without warranty of any kind. IBM shall not be</span>
<span style="color: #75715e;"># liable for any damages arising out of your use of the sample programs.</span>

<span style="color: #75715e;"># Each copy or any portion of these sample programs or any derivative work, must</span>
<span style="color: #75715e;"># include a copyright notice as follows:</span>

<span style="color: #75715e;"># (C) Copyright IBM Corp. 2012-2013.</span>
<span style="color: #75715e;"># createsymlink.sh</span>
<span style="color: #75715e;"># The script will produce a symbolic link required by the</span>
<span style="color: #75715e;"># IBM Platform LSF elim for NVIDIA GPUs.</span>
<span style="color: #75715e;">#</span>
<span style="color: #75715e;">#!/bin/sh</span>
LIBNVIDIA<span style="color: #f92672;">=</span><span style="color: #e6db74;">"/usr/lib64/nvidia/libnvidia-ml.so"</span>
<span style="color: #66d9ef;">if</span> <span style="color: #f92672;">[</span> -a <span style="color: #e6db74;">"</span>$LIBNVIDIA<span style="color: #e6db74;">"</span> <span style="color: #f92672;">]</span>
<span style="color: #66d9ef;">then</span>  
ln -s /usr/lib64/nvidia/libnvidia-ml.so /usr/lib64/libnvidia-ml.so
/etc/init.d/lsf stop
/etc/init.d/lsf start
<span style="color: #66d9ef;">fi</span>
exit <span style="color: #ae81ff;">0</span></code></pre></div>