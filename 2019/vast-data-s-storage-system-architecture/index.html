<!DOCTYPE html>
<html lang="en-US" data-theme="light">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>VAST Data's storage system architecture - hpc.social - Aggregated Personal Blog</title>
        <meta name="description" content="Shared personal experiences and stories">
        <meta name="keywords" content="">
        <base href="https://hpc.social/personal-blog" />
        
        <meta content="2019-02-27T05:23:00-07:00" property="article:published_time">
        <meta content="https://hpc.social/about/" property="article:author">
          
        <meta property="og:site_name" content="hpc.social - Aggregated Personal Blog">
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://hpc.social/personal-blog/2019/vast-data-s-storage-system-architecture/"/>
        <meta property="og:title" content="VAST Data's storage system architecture - hpc.social - Aggregated Personal Blog" />
        <meta property="og:description" content="Shared personal experiences and stories" />
        <meta property="og:image" content="https://hpc.social/personal-blog/assets/images/hpc-social-blue.png"/>
        <meta name="twitter:site" content="@hpc_social" />
        <meta name="twitter:creator" content="@hpc_social" /> 
        <meta name="twitter:card" content="summary"/>
        <meta property="twitter:title" content="VAST Data's storage system architecture - hpc.social - Aggregated Personal Blog" />
        <meta property="twitter:description" content="Shared personal experiences and stories" />
        <meta property="twitter:image" content="https://hpc.social/personal-blog/assets/images/hpc-social-blue.png" />
        <link rel="stylesheet" href="/personal-blog/assets/css/highlight.css">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Catamaran:wght@300&display=swap" rel="stylesheet"> 

        <link rel="stylesheet" href="/personal-blog/assets/css/style.css">
        <link rel="shortcut icon" href="/personal-blog/assets/images/hpc-social-blue.png" />
        <link rel="alternate" type="application/atom+xml" title="hpc.social - Aggregated Personal Blog" href="https://hpc.social/personal-blog/atom.xml">
        <link rel="alternate" type="application/json" title="hpc.social - Aggregated Personal Blog" href="https://hpc.social/personal-blog/feed.json" />
        <link rel="sitemap" type="application/xml" title="sitemap" href="https://hpc.social/personal-blog/sitemap.xml" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"hpc.social"},"description":"Shared personal experiences and stories","headline":"hpc.social - Aggregated Personal Blog","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://hpc.social/personal-blog/assets/images/hpc-social-blue.png"},"name":"hpc.social"},"sameAs":["https://twitter.com/","https://github.com/"],"url":"https://hpc.social/personal-blog"}</script>
    </head>
<body>

<div class="container">
	<div class="profile">
		<a href="https://hpc.social/personal-blog/"><img src="/personal-blog/assets/images/hpc-social-blue.png" class="profile-image"></a>
		<div class="profile-about">
			<h2 style="margin-bottom: 0; font-weight: 700;">hpc.social</h2>
			
			
				<a href="https://twitter.com/hpc_social" target="_blank"><img src="/personal-blog/assets/images/icon/twitter.svg" class="social-icon"></a>
			
			
			
			
				<a href="https://github.com/hpc-social" target="_blank"><img src="/personal-blog/assets/images/icon/github.svg" class="social-icon"></a>
			
			
			<a href="/personal-blog/about/"><img src="/personal-blog/assets/images/icon/me.svg" class="social-icon"></a>
			<br>
			High Performance Computing <br> Practitioners <br> and friends /#hpc
			<br>
			<div class="mode" id="mode-switcher" onclick="toggleNightMode();">
				<span></span>
			</div>
		</div>
	</div>
	
<div class="post-header">
	<div class="post-date">27.02.2019  &middot; <span class="reading-time" title="Estimated read time">
  
   35 mins  read </span>
</div>
	<div class="post-share">
		Share:&nbsp; 
		<a href="https://twitter.com/intent/tweet?source=tweetbutton&amp;original_referer=https://hpc.social/vast-data-s-storage-system-architecture&amp;text=VAST Data's storage system architecture - https://hpc.social/vast-data-s-storage-system-architecture" target="_blank"><img src="https://hpc.social/personal-blog/assets/images/icon/twitter.svg" class="social-icon"></a>
	</div>
</div>

<div class="tags-container" style="padding:10px; font-size:18px; font-style:italic">
        This is a crosspost from &nbsp; <a target="_blank" href="https://glennklockwood.blogspot.com/search/label/hpc">Glenn K. Lockwood</a>&nbsp;Personal thoughts and opinions of a supercomputing enthusiast.&nbsp;See the original post&nbsp;<a target="_blank" href="https://glennklockwood.blogspot.com/2019/02/vast-datas-storage-system-architecture.html">here</a>.</div>

<div class="blog-post-content">
	<h1>VAST Data's storage system architecture</h1>
	<p><a href="https://www.vastdata.com/">VAST Data, Inc</a>, an interesting new storage company, unveiled their new all-flash storage system today amidst a good amount of hype and fanfare.  There’s no shortage of marketing material and trade press coverage out there about their company and the juiciest features of their storage architecture, so to catch up on what all the talk has been about, I recommend taking a look at<br />&lt;div&gt;&lt;ul&gt;&lt;li&gt;The <a href="https://www.vastdata.com/app/pdf/datasheet.pdf">VAST “Universal Storage” datasheet</a>&lt;/li&gt;&lt;li&gt;The Next Platform’s article, “<a href="https://www.nextplatform.com/2019/02/26/vast-data-clustered-flash-storage-bans-the-disk-from-the-datacenter/">VAST Data Clustered Flash Storage Bans The Disk From The Datacenter</a>”&lt;/li&gt;&lt;li&gt;Chris Mellor’s piece, “<a href="https://blocksandfiles.com/2019/02/26/vast-datas-extinction-level-event-for-disk-drives-and-tiering/">VAST Data: The first thing we do, let’s kill all the hard drives</a>”&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</p>
<div>The reviews so far are quite sensational in the literal sense since VAST is one of very few storage systems being brought to market that have been designed from top to bottom to use modern storage technologies (containers, NVMe over Fabrics, and byte-addressable non-volatile memory) <i>and</i> tackle the harder challenge of file-based (not block-based) access.</div>
<div><div><br /></div>
<div>In the interests of grounding the hype in reality, I thought I would share various notes I've jotted down based on my understanding of the VAST architecture. &nbsp;That said, I have to make a few disclaimers up front:</div>
<div><ol><li>I have no financial interests in VAST, I am not a VAST customer, I have never tested VAST, and everything I know about VAST has come from just a few conversations with a limited number of people in the company. &nbsp;This essentially means I have no idea what I'm talking about.</li><li>I do not have any NDAs with VAST and none of this material is confidential. &nbsp;Much of it is from public sources now. &nbsp;I am happy to provide references where possible. &nbsp;If you are one of my sources and want to be cited or credited, please let me know.</li><li>These views represent my own personal opinions and not those of my employer, sponsors, or anyone else.</li></ol></div>
<div>With that in mind, what follows is a semi-coherent overview of the VAST storage system as I understand it. &nbsp;If you read anything that is wrong or misguided, rest assured that it is not intentional. &nbsp;Just let me know and I will be more than happy to issue corrections (and provide attribution if you so desire).</div>
</div>
<div><br />(<b>Update on May 12, 2020</b>: There is now an <a href="https://vastdata.com/whitepaper">authoritative whitepaper on how VAST works under the hood</a> on the VAST website. &nbsp;Read that, especially "How It Works," for a better informed description than this post.)<br /><br /></div>
<div><h2>Relevant Technologies</h2><div>A VAST storage system is comprised of two flavors of building blocks:</div>
<div><ol><li><b>JBOFs</b> (VAST calls them "d boxes" or "HA enclosures"). &nbsp;These things are what contain the storage media itself.</li><li><b>I/O servers</b> (VAST calls them "cnodes," "servers," "gateways" or, confusingly, "compute nodes"). &nbsp;These things are what HPC cluster compute nodes talk to to perform I/O via NFS or S3.</li></ol></div>
<div>Tying these two building blocks together is an RDMA fabric of some sort--either InfiniBand or RoCE. &nbsp;Conceptually, it would look something like this:</div>
<div><br /></div>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-fwRspws0SAk/XHYc4Oj6tjI/AAAAAAABD98/oxVEAYQMJgAUGgSlX7CfjLokiCHo-Yt0QCLcBGAs/s1600/The%2BVAST%2BData%2BArchitecture.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="338" src="https://2.bp.blogspot.com/-fwRspws0SAk/XHYc4Oj6tjI/AAAAAAABD98/oxVEAYQMJgAUGgSlX7CfjLokiCHo-Yt0QCLcBGAs/s400/The%2BVAST%2BData%2BArchitecture.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Conceptual diagram of how VAST Data's storage system (IOS, storage fabric, and JBOFs) might fit into a canonical HPC system. &nbsp;Interestingly, strongly resembles old-school block-based SAN architectures.</td></tr></tbody></table><div><br /></div>
<div>For the sake of clarity, we'll refer to the HPC compute nodes that run applications and perform I/O through an NFS client as "clients" hereafter. &nbsp;We'll also assume that all I/O to and from VAST occurs using NFS, but remember that VAST also supports S3.</div>
<div><br /></div>
<h3>JBOFs</h3><div>JBOFs are dead simple and their only job is to expose each NVMe device attached to them as an NVMe over Fabrics (NVMeoF) target. &nbsp;They are not truly JBOFs because they do have (from <a href="https://www.vastdata.com/app/pdf/datasheet.pdf">the VAST spec sheet</a>):</div>
<div><ol><li>2x embedded active/active servers, each with two Intel CPUs and the necessary hardware to support failover</li><li>4x 100 gigabit NICs, either operating using RoCE or InfiniBand</li><li>38x 15.36 TB U.2 SSD carriers. &nbsp;These are actually carriers that take multiple M.2 SSDs.</li><li>18x 960 GB U.2 Intel Optane SSDs</li></ol></div>
<div>However they are not intelligent. &nbsp;They are not RAID controllers nor do they do <i>any</i> data motion between the SSDs they host. &nbsp;They literally serve each device out to the network and that's it.</div>
<div><br /></div>
<h3>I/O Servers</h3><div>I/O servers are where the magic happens, and they are physically discrete servers that&nbsp;</div>
<div><ol><li>share the same SAN fabric as the JBOFs and speak NVMeoF on one side, and</li><li>share a network with client nodes and talk NFS on the other side</li></ol></div>
<div>These I/O servers are completely stateless; all the data stored by VAST is stored in the JBOFs. &nbsp;The I/O servers have no caches; their job is to turn NFS requests from compute nodes into NVMeoF transfers to JBOFs. &nbsp;Specifically, they perform the following functions:</div>
<div><ol><li>Determine which NVMeoF device(s) to talk to to serve an incoming I/O request from an NFS client. &nbsp;This is done using a hashing function.</li><li>Enforce file permissions, ACLs, and everything else that an NFS client would expect.</li><li>Transfer data to/from SSDs, and transfer data to/from 3D XPoint drives.</li><li>Transfer data between SSDs and 3D XPoint drives. &nbsp;This happens as part of the regular write path, to be discussed later.</li><li>Perform "global compression" (discussed later), rebuilds from parity, and other maintenance tasks.</li></ol></div>
<div>It is also notable that I/O servers do not have an affinity to specific JBOFs as a result of the hash-based placement of data across NVMeoF targets. &nbsp;They are all simply stateless worker bees that process I/O requests from clients and pass them along to the JBOFs. &nbsp;As such, they do not need to communicate with each other or synchronize in any way.</div>
<div><br /></div>
<h3>System Composition</h3><div>Because I/O servers are stateless and operate independently, they can be dynamically added (and removed) from the system at any time to increase or decrease the I/O processing power available to clients. &nbsp;VAST's position is that the peak I/O performance to the JBOFs is virtually always CPU limited since the data path between CPUs (in the I/O servers) and the storage devices (in JBOFs) uses NVMeoF. &nbsp;This is a reasonable assertion since NVMeoF is extremely efficient at moving data as a result of its use of RDMA and simple block-level access semantics.</div>
<div><br /></div>
<div>At the same time, this design requires that every I/O server be able to communicate with every SSD in the entire VAST system via NVMeoF. &nbsp;This means that each I/O server mounts every SSD at the same time; in a relatively small two-JBOF system, this results in 112x NVMe targets on every I/O server. &nbsp;This poses two distinct challenges:</div>
<div><ol><li>From an implementation standpoint, this is <b>pushing the limits of how many NVMeoF targets a single Linux host can effectively manage</b> in practice. &nbsp;For example, a 10 PB VAST system will have over 900 NVMeoF targets mounted on every single I/O server. &nbsp;There is no fundamental limitation here, but this scale will exercise pieces of the Linux kernel in ways it was never designed to be used.</li><li>From a fundamental standpoint, this <b>puts tremendous pressure on the storage network</b>. &nbsp;Every I/O server has to talk to every JBOF as a matter of course, resulting in a network dominated by all-to-all communication patterns. &nbsp;This will make performance extremely sensitive to topology, and while I wouldn't expect any issues at smaller scales, high-diameter fat trees will likely see these sensitivities manifest. &nbsp;The Lustre community turned to fine-grained routing to counter this exact issue on fat trees. &nbsp;Fortunately, InfiniBand now has adaptive routing that I expect will bring much more forgiveness to this design.</li></ol></div>
</div>
<div>This said, VAST has tested their architecture at impressively large scale and has an aggressive scale-out validation strategy.</div>
<div><br /></div>
<h3>Shared-everything consistency</h3>
<div>Mounting every block device on every server may also sound like anathema to anyone familiar with block-based SANs, and generally speaking, it is. &nbsp;NVMeoF (and every other block-level protocol) does not really have locking, so if a single device is mounted by two servers, it is up to those servers to communicate with each other to ensure they aren't attempting to modify the same blocks at the same time. &nbsp;Typical shared-block configurations manage this by simply assigning exclusive ownership of each drive to a single server and relying on heartbeating or quorum (e.g., in HA enclosures or GPFS) to decide when to change a drive's owner. &nbsp;StorNext (formerly CVFS) allows all clients to access all devices, but it uses a central metadata server to manage locks.</div>
<div><br /></div>
<div>VAST can avoid a lot of these problems by simply not caching any I/Os on the I/O servers and instead passing NFS requests through as NVMeoF requests. &nbsp;This is not unlike how parallel file systems like PVFS (now OrangeFS) avoided the lock contention problem; not using caches dramatically reduces the window of time during which two conflicting I/Os can collide. &nbsp;VAST also claws back some of the latency penalties of doing this sort of direct I/O by issuing all writes to nonvolatile memory instead of flash; this will be discussed later.</div>
<div><br /></div>
<div>For the rare cases where two I/O servers are asked to change the same piece of data at the same time though, there is a mechanism by which an extent of a file (which is on the order of 4 KiB) can be locked. &nbsp;I/O servers will flip a lock bit for that extent in the JBOF's memory using an atomic RDMA operation before issuing an update to serialize overlapping I/Os to the same byte range. &nbsp;</div>
<div><br /></div>
<div>VAST also uses redirect-on-write to ensure that writes are always consistent. &nbsp;If a JBOF fails before an I/O is complete, presumably any outstanding locks evaporate since they are resident only in RAM. &nbsp;Any changes that were in flight simply get lost because the metadata structure that describes the affected file's layout only points to updated extents after they have been successfully written. &nbsp;Again, this redirect-on-complete is achieved using an atomic RDMA operation, so data is always consistent. VAST does not need to maintain a write journal as a result.</div>
<div><br /></div>
<div>It is not clear to me what happens to locks in the event that an I/O server fails while it has outstanding I/Os. &nbsp;Since I/O servers do not talk to each other, there is no means by which they can revoke locks or probe each other for timeouts. &nbsp;Similarly, JBOFs are dumb, so they cannot expire locks.</div>
<div><br /></div>
<h2>The VAST write path</h2>
<div>I think the most meaningful way to demonstrate how VAST employs parity and compression while maintaining low latency is to walk through each step of the write path and show what happens between the time an application issues a write(2) call and the time that write call returns.</div>
<div><br /></div>
<div>First, an application on a compute node issues a write(2) call on an open file that happens to reside on an NFS mount that points to a VAST server. &nbsp;That write flows through the standard Linux NFS client stack and eventually results in an NFS RPC being sent over the wire to a VAST server. &nbsp;Because VAST clients use the standard Linux NFS client there are a few standard limitations. &nbsp;For example,<br /><ol><li>There is no parallel I/O from the client. &nbsp;A single client cannot explicitly issue writes to multiple I/O servers. &nbsp;Instead, some sort of <a href="https://www.emc.com/collateral/hardware/white-papers/h8316-wp-smartconnect.pdf">load balancing technique</a> must be inserted between the client and servers.</li><li>VAST violates POSIX because it only ensures NFS close-to-open consistency. &nbsp;If two compute nodes try to modify the same 4 KiB range of the same file at the same time, the result will be corrupt data. &nbsp;VAST's server-side locking cannot prevent this because it happens at the client side. &nbsp;The best way around this is to force all I/O destined to a VAST file system to use direct I/O (e.g., open with O_DIRECT).</li></ol><div>Pictorially, it might look something like this:</div>
<div><br /></div>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-u0n8JTcmsYk/XHd-DF6RrvI/AAAAAAABD-M/Iy8PRzE0Yootom8IkmLkNoA8-i5YlWpwgCLcBGAs/s1600/vast-write-1.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="323" src="https://2.bp.blogspot.com/-u0n8JTcmsYk/XHd-DF6RrvI/AAAAAAABD-M/Iy8PRzE0Yootom8IkmLkNoA8-i5YlWpwgCLcBGAs/s640/vast-write-1.png" width="100%" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Step 1 of VAST write path: client issues a standard NFS RPC to a VAST I/O server</td></tr></tbody></table><div>Then the VAST I/O server receives the write RPC and has to figure out to which NVMeoF device(s) the data should be written. &nbsp;This is done by first determining on which NVMe device the appropriate file's metadata is located. &nbsp;This metadata is stored in B-tree like data structures with a very wide fan-out ratio and whose roots are mapped to physical devices algorithmically. &nbsp;Once an I/O server knows which B-tree to begin traversing to find a specific file's metadata algorithmically, it begins traversing that tree to find the file, and then find the location of that file's extents. &nbsp;The majority of these metadata trees live in 3D XPoint, but very large file systems may have their outermost levels stored in NAND.</div>
<div><br /></div>
<div>A key aspect of VAST's architecture is that writes always land on 3D XPoint first; this narrows down the possible NVMeoF targets to those which are storage-class memory devices.<br /><br />Pictorially, this second step may look something like this:<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-2abTaRik0t0/XHd_M0XVBlI/AAAAAAABD-U/zwLZSNXiNfEUEVyGzDskuU_bMMjCJcUSQCLcBGAs/s1600/vast-write-2.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="323" src="https://3.bp.blogspot.com/-2abTaRik0t0/XHd_M0XVBlI/AAAAAAABD-U/zwLZSNXiNfEUEVyGzDskuU_bMMjCJcUSQCLcBGAs/s640/vast-write-2.png" width="100%" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Step 2 of VAST write path: I/O server forwards write to 3D XPoint devices. &nbsp;Data is actually triplicated at this point for reasons that will be explained later.</td></tr></tbody></table>VAST uses 3D XPoint for two distinct roles:</div>
<div><ol><li>Temporarily store all incoming writes</li><li>Store the metadata structures used to describe files and where the data for files reside across all of the NVMe devices</li></ol><div>VAST divides 3D XPoint used for case #1 into buckets. &nbsp;Buckets are used to group data based on how long that data is expected to persist before being erased; incoming writes that will be written once and never erased go into one bucket, while incoming writes that may be overwritten (erased) in a very short time will go into another. &nbsp;VAST is able to make educated guesses about this because it knows many user-facing features of the file (its parent directory, extension, owner, group, etc) to which incoming writes are being written, and it tracks file volatility over time.</div>
</div>
<div><br /></div>
<div>Data remains in a 3D XPoint bucket until that bucket is full. &nbsp;The bucket is full when its size can be written to the NAND SSDs such that entire SSD erase blocks (which VAST claims can be on the order of a gigabyte in size) can be written down to NAND at once. &nbsp;Since JBOFs are dumb, this actually results in I/O servers reading back a full bucket out of 3D XPoint:<br /><div class="separator" style="clear: both; text-align: center;"></div>
<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-RJGVUcyTBiE/XHeAKVtyEFI/AAAAAAABD-k/PBimD6KGY5YVIBgiaQtp7pLKnr9kTFhUACLcBGAs/s1600/vast-write-3.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="323" src="https://3.bp.blogspot.com/-RJGVUcyTBiE/XHeAKVtyEFI/AAAAAAABD-k/PBimD6KGY5YVIBgiaQtp7pLKnr9kTFhUACLcBGAs/s640/vast-write-3.png" width="100%" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Step 3 of VAST write path: Once sufficient writes have been received to fill a bucket and create a full stripe, the I/O server must read it from 3D XPoint. &nbsp;Note that this diagram may be misleading; it is unclear if a single bucket resides on a single 3D XPoint device, or if a bucket is somehow sharded. &nbsp;My guess is the former (as shown).</td></tr></tbody></table>The I/O server then bounces that bucket back out to NAND devices:<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-tNBnyVf_y3Y/XHeBQ2sXi9I/AAAAAAABD-s/0coxApTRWvoeUBhSmplAgzaiufm-gSqBwCLcBGAs/s1600/vast-write-4.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="323" src="https://3.bp.blogspot.com/-tNBnyVf_y3Y/XHeBQ2sXi9I/AAAAAAABD-s/0coxApTRWvoeUBhSmplAgzaiufm-gSqBwCLcBGAs/s640/vast-write-4.png" width="100%" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Step 4 of VAST write path: Once a full stripe has been formed in 3D XPoint and the I/O node has read it into DRAM, it actually writes that stripe down across many NAND devices. &nbsp;Again, this diagram is probably inaccurate as a result of my own lack of understanding; the relationship between a bucket (which maps to a single SSD's erase block) and a stripe (which must touch N+M SSDs) is unclear to me.</td></tr></tbody></table>By writing out an entire erase block at once, VAST avoids the need for the SSD to garbage collect and amplify writes, since erase blocks are never only partially written. &nbsp;Erase blocks are also presumably rarely (or never?) only partially erased either; this is a result of</div>
<div><ol><li>the combined volatility-based bucketing of data (similarly volatile data tends to reside in the same erase block), and</li><li>VAST's redirect-on-write nature (data is never overwritten; updated data is simply written elsewhere and the file's metadata is updated to point to the new data).</li></ol><div>Because VAST relies on cheap consumer NAND SSDs, the data is not safe in the event of a power loss even after the NAND SSD claims the data is persisted. &nbsp;As a result, VAST then forces each NAND SSD to flush its internal caches to physical NAND. &nbsp;Once this flush command returns, the SSDs have guaranteed that the data is power fail-safe. &nbsp;VAST then deletes the bucket contents from 3D XPoint:</div>
</div>
<div><br /></div>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-bsKdxL1chJs/XHeDA8F0cJI/AAAAAAABD-4/FK-cCC1S6DYZOOa73C_Iq2G1QDbdrmrDwCLcBGAs/s1600/vast-write-5.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="324" src="https://1.bp.blogspot.com/-bsKdxL1chJs/XHeDA8F0cJI/AAAAAAABD-4/FK-cCC1S6DYZOOa73C_Iq2G1QDbdrmrDwCLcBGAs/s640/vast-write-5.png" width="100%" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Step 5 of the VAST write path: Once data is truly persisted and safe in the event of power loss, VAST purges the original copy of that bucket that resides on the 3D XPoint.</td></tr></tbody></table><div>The metadata structures for all affected files are updated to point at the version of the data that now resides on NAND SSDs, and the bucket is free to be filled by the next generation of incoming writes.<br /><br /></div>
<h3>Data Protection</h3><div>These large buckets also allow VAST to use extremely wide striping for data protection. &nbsp;As writes come in and fill buckets, large stripes are also being built with a minimum of 40+4 parity protection. &nbsp;Unlike in a traditional RAID system where stripes are built in memory, VAST's use of nonvolatile memory (3D XPoint) to store partially full buckets allow very wide stripes to be built over larger windows of time without exposing the data to loss in the event of a power failure. &nbsp;Partial stripe writes never happen because, by definition, a stripe is only written down to flash once it is full.</div>
<div><br /></div>
<div>Bucket sizes (and by extension, stripe sizes) are variable and dynamic. &nbsp;VAST will opportunistically write down a stripe as erase blocks become available. &nbsp;As the number of NVMe devices in the VAST system increases (e.g., more JBOFs are installed), stripes can become wider. &nbsp;This is advantageous when one considers the erasure coding scheme that VAST employs; rather than use a Reed-Solomon code, they have developed their own parity algorithm that allows blocks to be rebuilt from only a subset of the stripe. &nbsp;An example stated by VAST is that a 150+4 stripe only requires 25% of the remaining data to be read to rebuild. &nbsp;As pointed out by <a href="https://glennklockwood.blogspot.com/2019/02/vast-datas-storage-system-architecture.html?showComment=1551464397653#c6795285534302272414">Shuki Bruck though</a>, this is likely a derivative of the <a href="https://doi.org/10.1109/TIT.2012.2227110">Zigzag coding scheme introduced by Tamo, Wang, and Bruck in 2013</a>, where a data coded using N+M parity only require (N+M)/M reads to rebuild.</div>
<div><br /></div>
<div>To summarize, parity-protected stripes are slowly built in storage-class memory over time from bits of data that are expected to be erased at roughly the same time. &nbsp;Once a stripe is fully built in 3D XPoint, it is written down to the NAND devices. &nbsp;As a reminder, I/O servers are responsible for moderating all of this data movement and parity generation; the JBOFs are dumb and simply offer up the 3D XPoint targets.</div>
<div><br /></div>
<div>To protect data as stripes are being built, the contents of the 3D XPoint layer are simply triplicated. &nbsp;This is to say that every partially built stripe's contents appear on three different 3D XPoint devices.<br /><br /></div>
<h3>Performance Expectations</h3><div>This likely has a profound effect on the write performance of VAST; if a single 1 MB write is issued by an NFS client, the I/O server must write 3 MB of data to three different 3D XPoint devices. &nbsp;While this would not affect latency by virtue of the fact that the I/O server can issue NVMeoF writes to multiple JBOFs concurrently, this means that the NICs facing the backend InfiniBand fabric must be able to inject data three times as fast as data arriving from the front-end, client-facing network. &nbsp;<b>Alternatively, VAST is likely to carry an intrinsic 3x performance penalty to writes versus reads.</b></div>
<div><br /></div>
<div>There are several factors that will alter this in practice:</div>
<div><ul><li>Both 3D XPoint SSDs and NAND SSDs have higher read bandwidth than write bandwidth as a result of the power consumption associated with writes. &nbsp;This will further increase the 3:1 read:write performance penalty.</li><li>VAST always writes to 3D XPoint but may often read from NAND. &nbsp;This closes the gap in theory, since 3D XPoint is significantly faster at both reads and writes than NAND is at reads in most cases. &nbsp;However the current 3D XPoint products on the market are PCIe-attached and limited to PCIe Gen3 speeds, so there is not a significant bandwidth advantage to 3D XPoint writes vs. NAND reads.</li></ul><div>It is also important to point out that VAST has yet to publicly disclose any performance numbers. &nbsp;However, using replication to protect writes is perhaps the only viable strategy to deliver extremely high IOPS without sacrificing data protection. &nbsp;WekaIO, which also aims to deliver extremely high IOPS, showed a similar 3:1 read:write performance skew in <a href="https://www.vi4io.org/io500/list/19-01/10node">their IO-500 submission in November</a>. &nbsp;While WekaIO uses a very different approach to achieving low latency at scale, their benchmark numbers indicate that scalable file systems that optimize for IOPS are likely to sacrifice write throughput to achieve this. &nbsp;VAST's architecture and choice to replicate writes is in line with this expectation, but until VAST publishes performance numbers, this is purely speculative. &nbsp;I would like to be proven wrong.<br /><br /><h2>Other Bells and Whistles</h2>The notes presented above are only a small part of the full VAST architecture, and since I am no expert on VAST, I'm sure there's even more that I don't realize I don't know or fully understand. &nbsp;That said, I'll highlight a few examples of which I am tenuously aware:<br /><br />Because every I/O server sees every NVMe device, it can perform <b>global compression</b>. &nbsp;Typical compression algorithms are designed only to compress adjacent data within a fixed block size, which means similar but physically disparate blocks cannot be reduced. &nbsp;VAST tracks a similarity value for extents in its internal metadata and will group these similar extents before compressing them. &nbsp;I envision this to work something like a Burrows-Wheeler transformation (it is definitely not one though) and conceptually combines the best features of compression and deduplication. &nbsp;I have to assume this compression happens somewhere in the write path (perhaps as stripes are written to NAND), but I don't understand this in any detail.<br /><br />The exact compression algorithm is one of VAST's own design, and it is not block-based as a result of VAST not having a fixed block size. &nbsp;This means that decompression is also quite different from block-based compression; according to VAST, their algorithm can decompress only a local subset of data such that reads do not require similar global decompression. &nbsp;The net result is that read performance of compressed data is not significantly compromised. &nbsp;VAST has a very compelling example where they compressed data that was already compressed and saw a significant additional capacity savings as a result of the global nature of their algorithm. &nbsp;While I normally discount claims of high compression ratios since they never hold up for scientific data, the conceptual underpinnings of VAST's approach to compression sounds promising.<br /><br />VAST is also very closely tied to byte-addressable nonvolatile storage from top to bottom, and much of this is a result of their <b>B-tree-based file system metadata structure</b>. &nbsp;They refer to their underlying storage substrate as an "element store" (which I imagine to be similar to a key-value store), and it sounds like it is designed to store a substantial amount of metadata per file. &nbsp;In addition to standard POSIX metadata and the pointers to data extents on various NVMe devices, VAST also stores user metadata (in support of their S3 interface) and internal metadata (such as heuristics about file volatility, versioning for continuous snapshots, etc). &nbsp;This element store API is not exposed to customers, but it sounds like it is sufficiently extensible to support a variety of other access APIs beyond POSIX and S3.<br /><br /><h2>Take-away Messages</h2>VAST is an interesting new all-flash storage system that resulted from taking a green-field approach to storage architecture. &nbsp;It uses a number of new technologies (storage-class memory/3D XPoint, NAND, NVMe over fabrics) in intellectually satisfying ways, and builds on them using a host of byte-granular algorithms. &nbsp;It looks like it is optimized for both cost (in its intelligent optimization of flash endurance) and latency (landing I/Os on 3D XPoint and using triplication) which have been traditionally difficult to optimize together.<br /><br />Its design does rely on an extremely robust backend RDMA fabric, and the way in which every I/O server must mount every storage device sounds like a path to scalability problems--both in terms of software support in the Linux NVMeoF stack and fundamental sensitivities to topology inherent in large, high-diameter RDMA fabrics. &nbsp;The global all-to-all communication patterns and choice to triplicate writes make the back-end network critically important to the overall performance of this architecture.<br /><br />That said, the all-to-all ("shared everything") design of VAST brings a few distinct advantages as well. &nbsp;As the system is scaled to include more JBOFs, the global compression scales as well and can recover an increasing amount of capacity. &nbsp;Similarly, data durability increases as stripes can be made wider and be placed across different failure domains. &nbsp;In this sense, the efficiency of the system increases as it gets larger due to the global awareness of data. &nbsp;VAST's choice to make the I/O servers stateless and independent also adds the benefit of being able to scale the front-end capability of the system independently of the back-end capacity. &nbsp;Provided the practical and performance challenges of scaling out described in the previous paragraph do not manifest in reality, this bigger-is-better design is an interesting contrast to the mass storage systems of today which, at best, do not degrade as they scale out. &nbsp;Unfortunately, VAST has not disclosed any performance or scaling numbers, so the proof will be in the pudding.<br /><br />However, VAST has hinted that the costs are "one fifth to one eighth" of enterprise flash today; by their own estimates of today's cost of enterprise flash, this translates to a cost of between $0.075 and $0.12 per gigabyte of flash when deployed in a VAST system. &nbsp;This remains 3x-5x more expensive than spinning disk today, but the cost of flash is dropping far faster than the cost of hard drives, so the near-term future may truly make VAST cost-comparable to disk. &nbsp;As flash prices continue to plummet though, the VAST cost advantage may become less dramatic over datacenter flash, but their performance architecture will remain compelling when compared to a traditional disk-oriented networked file system.<br /><br />As alluded above, VAST is not the first company to develop a file-based storage system designed specifically for flash, and they share many similar architectural design patterns with their competition. &nbsp;This is creating gravity around a few key concepts:<br /><ul><li>Both flash and RDMA fabrics handle kilobyte-sized transfers with grace, so the days of requiring megabyte-sized I/Os to achieve high bandwidth are nearing an end.</li><li>The desire to deliver high IOPS makes replication an essential part of the data path which will skew I/O bandwidth towards reads. &nbsp;This maps well for read-intensive workloads such as those generated by AI, but this does not bode as well for write-intensive workloads of traditional modeling and simulation.</li><li>Reserving CPU resources exclusively for driving I/O is emerging as a requirement to get low-latency and predictable I/O performance with kilobyte-sized transfers. &nbsp;Although not discussed above, VAST uses containerized I/O servers to isolate performance-critical logic from other noise on the physical host. &nbsp;This pattern maps well to the notion that in exascale, there will be an abundance of computing power relative to the memory bandwidth required to feed computations.</li><li>File-based I/O is not entirely at odds with very low-latency access, but this file-based access is simple one of many interfaces exposed atop a more flexible key-value type of data structure. &nbsp;As such, as new I/O interfaces emerge to serve the needs of extremely latency-sensitive workloads, these flexible new all-flash storage systems can simply expose their underlying performance through other non-POSIX APIs.</li></ul><div>Finally, if you've gotten this far, it is important to underscore that I am in no way speaking authoritatively about anything above. &nbsp;If you are really interested in VAST or related technologies, don't take it from me; talk to the people and companies developing them directly.</div>
</div>
</div>
</div>

</div>
<div class="tags-container">
	
</div>
<style>
#uppy {
  position: fixed;
  bottom: 50px;
  right: 50px;
  z-index: 99;
  border: none;
  outline: none;
  background-color: darkturquoise;
  color: black;
  cursor: pointer;
  margin:15px;
  border-radius: 10px;
  width: 100px;
  height: 20px;
}

#returnTop:hover {
  background-color: #555;
}
</style>

<button id="uppy" title="Return to Top" class="btn btn-lg">Back to top</button>

<script>
(function() {

var scrollTop = function() {
    window.scrollTo(0, 0);
};

document.getElementById('uppy').onclick = scrollTop;

})();
</script>


<div class="navigation">
	
		<a class="prev" href="/personal-blog/2019/what-should-a-national-research-computing-platform-be/">< What Should a National Research Computing Platform Be?</a>
	 
	<a class="next" href="/personal-blog/2019/powering-the-future-of-hpc-ai-with-openpower/">Powering the Future of HPC & AI with OpenPOWER ></a>

</div>

	<div class="footer">
		<span style="padding-right:10px"><font color="red">©️ </font><a href="https://hpc.social"><b>hpc.social</b> community</a></span> 
		| <a style="padding-left:10px; padding-right:10px" href="/personal-blog/archive/">Archive</a>
		| <a style="padding-left:10px; padding-right:10px" href="/personal-blog/">Posts</a>
		| <a style="padding-left:10px; padding-right:10px" href="/personal-blog/about/">About</a>
		| <a style="padding-left:10px; padding-right:10px" href="https://hpc.social/blog">hpc.social blogs</a>
		| <a style="padding-left:37%;" target="_blank" href="https://github.com/hpc-social/personal-blog">Add Your Blog</a>
	</div>
</div>
    <script src="/personal-blog/assets/scripts.js"></script>
    <script type="text/javascript">
        if (localStorage.theme === 'light') {
            document.documentElement.setAttribute('data-theme', 'light');
            document.getElementById('mode-switcher').classList.add('active');
        } else if (localStorage.theme === 'dark' || localStorage.theme === '' || (!('theme' in localStorage))) {
            document.documentElement.setAttribute('data-theme', 'dark');
            document.getElementById('mode-switcher').classList.add('active');
        }
    </script>
</body>
</html>

